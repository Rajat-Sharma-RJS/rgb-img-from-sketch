{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sketch_Synthesis_&Style_guided_S2I_Synthesis.ipynb","provenance":[{"file_id":"1yIEu_yICdh2B3MiXK4nyXu07V_PRdgmG","timestamp":1634111663777},{"file_id":"1ZabENj2LLxLk5K484WtYNL_tChcPrpMD","timestamp":1631506152616}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1860d5dbbeb846d98426f4b9883c2329":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7c4d892073144037bdd92c486323820f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0be574fcbc4e42549ec46857ba4cc64b","IPY_MODEL_4e8143c8bda048f98a7f40f933f426f3","IPY_MODEL_81d73d544d4c4dba90290f186c1e835a"]}},"7c4d892073144037bdd92c486323820f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0be574fcbc4e42549ec46857ba4cc64b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a779fb20c29d4b42a353417f2791093a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_39fd018d292342a3872f1b8dcf605346"}},"4e8143c8bda048f98a7f40f933f426f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bc06500725674dc6a24f862353b9e8be","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":553433881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e710cd41a4b40508efd43f470fa821e"}},"81d73d544d4c4dba90290f186c1e835a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0c91d8b3206c4207a88c1956589768a6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 528M/528M [00:05&lt;00:00, 95.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e3c88e85d8e45859f762c9e39125b4c"}},"a779fb20c29d4b42a353417f2791093a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"39fd018d292342a3872f1b8dcf605346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc06500725674dc6a24f862353b9e8be":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0e710cd41a4b40508efd43f470fa821e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0c91d8b3206c4207a88c1956589768a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4e3c88e85d8e45859f762c9e39125b4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"frp6URGlYORN"},"source":["## results are in the subfolder named \"saved\" of the drive folder :- https://drive.google.com/drive/folders/1_Nt3xnZJpVlI0jRJQM4QJ4vdMKjSSmZn?usp=sharing \n","## detailed result analysis :- https://docs.google.com/document/d/1nRHeqZVKCQ7sC9tGEDbVhdMrRtnR3Gv627Lfv6HEB2s/edit?usp=sharing \n","## output of training done for sketch synthesis is after the cell :- https://colab.research.google.com/drive/1u3LctUtFMHnXZlOUF-n2gN7yaUnZCht8#scrollTo=si5UIMg8hu_M&line=1&uniqifier=1 \n","## sketch generation is after this cell :- https://colab.research.google.com/drive/1u3LctUtFMHnXZlOUF-n2gN7yaUnZCht8#scrollTo=zRAcYGwxi5pn&line=1&uniqifier=1 \n","## training AutoEncoder for sketch-to-image synthesis is after this cell :- https://colab.research.google.com/drive/1u3LctUtFMHnXZlOUF-n2gN7yaUnZCht8#scrollTo=caaYS8KiygTo&line=1&uniqifier=1 \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hEdLxr34ZO3F"},"source":["import os\n","import time\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision.datasets as Dataset\n","import torchvision.utils as vutils\n","from torch import nn\n","from torch.utils.data import DataLoader\n","import argparse\n","import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNWqIC1XiHZ5","executionInfo":{"status":"ok","timestamp":1634107116419,"user_tz":-330,"elapsed":30448,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"ce80e824-eac4-437d-ec18-6cae10ef57d7"},"source":["from google.colab import drive\n","drive.mount('/content/My_drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/My_drive\n"]}]},{"cell_type":"code","metadata":{"id":"bpiO7HkvZgqQ"},"source":["import functools\n","from math import sqrt\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch import cat, sigmoid\n","from torch.autograd import Variable\n","from torch.nn import Parameter, init\n","from torch.nn.utils import spectral_norm\n","import torch.nn.functional as F\n","from torch.jit import ScriptModule, script_method, trace"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ypVlL5PZgnh"},"source":["def calc_mean_std(feat, eps=1e-5):\n","    size = feat.size()\n","    assert (len(size) == 4)\n","    N, C = size[:2]\n","    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n","    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n","    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n","    return feat_mean, feat_std\n","\n","\n","def adain(content_feat, style_feat):\n","    assert (content_feat.size()[:2] == style_feat.size()[:2])\n","    size = content_feat.size()\n","    style_mean, style_std = calc_mean_std(style_feat)\n","    content_mean, content_std = calc_mean_std(content_feat)\n","\n","    normalized_feat = (content_feat - content_mean.expand(\n","        size)) / content_std.expand(size)\n","    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n","\n","\n","def get_batched_gram_matrix(input):\n","    a, b, c, d = input.size()\n","    features = input.view(a, b, c * d)\n","    G = torch.bmm(features, features.transpose(2,1)) \n","    return G.div(b * c * d)\n","    \n","class Adaptive_pool(nn.Module):\n","    def __init__(self, channel_out, hw_out):\n","        super().__init__()\n","        self.channel_out = channel_out\n","        self.hw_out = hw_out\n","        self.pool = nn.AdaptiveAvgPool2d((channel_out, hw_out**2))\n","    def forward(self, input):\n","        if len(input.shape) == 3:\n","            input.unsqueeze_(1)\n","        return self.pool(input).view(-1, self.channel_out, self.hw_out, self.hw_out)\n","\n","class VGGSimple(nn.Module):\n","    def __init__(self):\n","        super(VGGSimple, self).__init__()\n","\n","        self.features = self.make_layers()\n","        \n","        self.norm_mean = torch.Tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n","        self.norm_std = torch.Tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n","\n","    def forward(self, img, after_relu=True, base=4):\n","        feat = (((img+1)*0.5) - self.norm_mean.to(img.device)) / self.norm_std.to(img.device)\n","        cut_points = [2, 7, 14, 21, 28]\n","        if after_relu:\n","            cut_points = [c+2 for c in cut_points]\n","        for i in range(31):\n","            feat = self.features[i](feat)\n","            if i == cut_points[0]:\n","                feat_64 = F.adaptive_avg_pool2d(feat, base*16)\n","            if i == cut_points[1]:\n","                feat_32 = F.adaptive_avg_pool2d(feat, base*8)\n","            if i == cut_points[2]:\n","                feat_16 = F.adaptive_avg_pool2d(feat, base*4)\n","            if i == cut_points[3]:\n","                feat_8 = F.adaptive_avg_pool2d(feat, base*2)\n","            if i == cut_points[4]:\n","                feat_4 = F.adaptive_avg_pool2d(feat, base)\n","        \n","        return feat_64, feat_32, feat_16, feat_8, feat_4\n","\n","    def make_layers(self, cfg=\"D\", batch_norm=False):\n","        cfg_dic = {\n","            'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","            'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","            'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","            'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","        }\n","        cfg = cfg_dic[cfg]\n","        layers = []\n","        in_channels = 3\n","        for v in cfg:\n","            if v == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","                if batch_norm:\n","                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=False)]\n","                else:\n","                    layers += [conv2d, nn.ReLU(inplace=False)]\n","                in_channels = v\n","        return nn.Sequential(*layers)\n","\n","\n","class VGG_3label(nn.Module):\n","    def __init__(self, nclass_artist=1117, nclass_style=55, nclass_genre=26):\n","        super(VGG_3label, self).__init__()\n","        self.features = self.make_layers()\n","        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        \n","        self.classifier_feat = self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(),\n","            nn.Linear(4096, 512))\n","\n","        self.classifier_style = nn.Sequential(nn.ReLU(), nn.Dropout(), nn.Linear(512, nclass_style))\n","        self.classifier_genre = nn.Sequential(nn.ReLU(), nn.Dropout(), nn.Linear(512, nclass_genre))\n","        self.classifier_artist = nn.Sequential(nn.ReLU(), nn.Dropout(), nn.Linear(512, nclass_artist))\n","\n","        self.norm_mean = torch.Tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n","        self.norm_std = torch.Tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n","    \n","        self.avgpool_4 = nn.AdaptiveAvgPool2d((4, 4))\n","        self.avgpool_8 = nn.AdaptiveAvgPool2d((8, 8))\n","        self.avgpool_16 = nn.AdaptiveAvgPool2d((16, 16))\n","    \n","    def get_features(self, img, after_relu=True, base=4):\n","        feat = (((img+1)*0.5) - self.norm_mean.to(img.device)) / self.norm_std.to(img.device)\n","        cut_points = [2, 7, 14, 21, 28]\n","        if after_relu:\n","            cut_points = [4, 9, 16, 23, 30]\n","        for i in range(31):\n","            feat = self.features[i](feat)\n","            if i == cut_points[0]:\n","                feat_64 = F.adaptive_avg_pool2d(feat, base*16)\n","            if i == cut_points[1]:\n","                feat_32 = F.adaptive_avg_pool2d(feat, base*8)\n","            if i == cut_points[2]:\n","                feat_16 = F.adaptive_avg_pool2d(feat, base*4)\n","            if i == cut_points[3]:\n","                feat_8 = F.adaptive_avg_pool2d(feat, base*2)\n","            if i == cut_points[4]:\n","                feat_4 = F.adaptive_avg_pool2d(feat, base)\n","        return feat_64, feat_32, feat_16, feat_8, feat_4\n","\n","\n","    def load_pretrain_weights(self):\n","        pretrained_vgg16 = vgg.vgg16(pretrained=True)\n","        self.features.load_state_dict(pretrained_vgg16.features.state_dict())\n","        self.classifier_feat[0] = pretrained_vgg16.classifier[0] \n","        self.classifier_feat[3] = pretrained_vgg16.classifier[3] \n","        for m in self.modules():\n","            if isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","    def make_layers(self, cfg=\"D\", batch_norm=False):\n","        cfg_dic = {\n","            'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","            'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","            'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","            'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","        }\n","        cfg = cfg_dic[cfg]\n","        layers = []\n","        in_channels = 3\n","        for v in cfg:\n","            if v == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","                if batch_norm:\n","                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=False)]\n","                else:\n","                    layers += [conv2d, nn.ReLU(inplace=False)]\n","                in_channels = v\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, img):\n","        feature = self.classifier_feat( self.avgpool(self.features(img)).view(img.size(0), -1) )\n","        pred_style = self.classifier_style(feature)\n","        pred_genre = self.classifier_genre(feature)\n","        pred_artist = self.classifier_artist(feature)\n","        return pred_style, pred_genre, pred_artist\n","\n","\n","class UnFlatten(nn.Module):\n","    def __init__(self, block_size):\n","        super(UnFlatten, self).__init__()\n","        self.block_size = block_size\n","\n","    def forward(self, x):\n","        return x.view(x.size(0), -1, self.block_size, self.block_size)\n","\n","\n","class Flatten(nn.Module):\n","    def __init__(self):\n","        super(Flatten, self).__init__()\n","\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","\n","class UpConvBlock(nn.Module):\n","    def __init__(self, in_channel, out_channel, norm_layer=nn.BatchNorm2d):\n","        super().__init__()\n","\n","        self.main = nn.Sequential(\n","            nn.ReflectionPad2d(1),\n","            spectral_norm(nn.Conv2d(in_channel, out_channel, 3, 1, 0, bias=True)),\n","            norm_layer(out_channel), \n","            nn.LeakyReLU(0.01), \n","            )\n","\n","    def forward(self, x):\n","        y = F.interpolate(x, scale_factor=2)\n","        return self.main(y)\n","\n","\n","class DownConvBlock(nn.Module):\n","    def __init__(self, in_channel, out_channel, norm_layer=nn.BatchNorm2d, down=True):\n","        super().__init__()\n","\n","        m = [   spectral_norm(nn.Conv2d(in_channel, out_channel, 3, 1, 1, bias=True)),\n","                norm_layer(out_channel), \n","                nn.LeakyReLU(0.1) ]\n","        if down:\n","            m.append(nn.AvgPool2d(2, 2))\n","        self.main = nn.Sequential(*m)\n","\n","    def forward(self, x):\n","        return self.main(x)\n","\n","\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self, infc=512, nfc=64, nc_out=3):\n","        super(Generator, self).__init__()\n","\n","        self.decode_32 = UpConvBlock(infc, nfc*4)\t\n","        self.decode_64 = UpConvBlock(nfc*4, nfc*4)    \n","        self.decode_128 = UpConvBlock(nfc*4, nfc*2)    \n","\n","        self.final = nn.Sequential(\n","            spectral_norm( nn.Conv2d(nfc*2, nc_out, 3, 1, 1, bias=True) ),\n","            nn.Tanh())\n","\n","    def forward(self, input):\n","\n","        decode_32 = self.decode_32(input)\n","        decode_64 = self.decode_64(decode_32)\n","        decode_128 = self.decode_128(decode_64)\n","\n","        output = self.final(decode_128)\n","        return output\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, nfc=512, norm_layer=nn.InstanceNorm2d):\n","        super(Discriminator, self).__init__()\n","\n","        self.main = nn.Sequential(\n","            DownConvBlock(nfc, nfc//2, norm_layer=norm_layer, down=False),\n","            DownConvBlock(nfc//2, nfc//4, norm_layer=norm_layer),\n","            spectral_norm( nn.Conv2d(nfc//4, 1, 4, 2, 0) )\n","        )\n","\t\n","    def forward(self, input):\n","        out = self.main(input)\n","        return out.view(-1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SzNiV9EBZgk9"},"source":["import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torchvision import transforms, utils\n","import subprocess as sp\n","from PIL import Image\n","import time\n","import torch.utils.data as data\n","\n","from skimage.color import hsv2rgb\n","import torch.nn.functional as F\n","import torch.nn as nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahXI-OA1Zgih"},"source":["eps = 1e-7\n","class HSV_Loss(nn.Module):\n","    def __init__(self, h=0, s=1, v=0.7):\n","        super(HSV_Loss, self).__init__()\n","        self.hsv = [h, s, v]\n","        self.l1 = nn.L1Loss()\n","        self.mse = nn.MSELoss()\n","        \n","    @staticmethod\n","    def get_h(im):\n","        img = im * 0.5 + 0.5\n","        b, c, h, w = img.shape\n","        hue = torch.Tensor(im.shape[0], im.shape[2], im.shape[3]).to(im.device)\n","        hue[img[:,2]==img.max(1)[0]] = 4.0+((img[:,0]-img[:,1])/(img.max(1)[0] - img.min(1)[0]))[img[:,2]==img.max(1)[0]]\n","        hue[img[:,1]==img.max(1)[0]] = 2.0+((img[:,2]-img[:,0])/(img.max(1)[0] - img.min(1)[0]))[img[:,1]==img.max(1)[0]]\n","        hue[img[:,0]==img.max(1)[0]] = ((img[:,1]-img[:,2])/(img.max(1)[0] - img.min(1)[0]))[img[:,0]==img.max(1)[0]]\n","        hue = (hue/6.0) % 1.0\n","        hue[img.min(1)[0]==img.max(1)[0]] = 0.0\n","        return hue \n","\n","    @staticmethod\n","    def get_v(im):\n","        img = im * 0.5 + 0.5\n","        b, c, h, w = img.shape\n","        it = img.transpose(1,2).transpose(2,3).contiguous().view(b, -1, c)        \n","        value = F.max_pool1d(it, c).view(b, h, w)\n","        return value \n","\n","    @staticmethod\n","    def get_s(im):\n","        img = im * 0.5 + 0.5\n","        b, c, h, w = img.shape\n","        it = img.transpose(1,2).transpose(2,3).contiguous().view(b, -1, c)        \n","        max_v = F.max_pool1d(it, c).view(b, h, w)\n","        min_v = F.max_pool1d(it*-1, c).view(b, h, w)\n","        satur = (max_v + min_v) / (max_v+eps)\n","        return satur\n","\n","    def forward(self, input):\n","        h = self.get_h(input)\n","        s = self.get_s(input)\n","        v = self.get_v(input)\n","        target_h = torch.Tensor(h.shape).fill_(self.hsv[0]).to(input.device).type_as(h)\n","        target_s = torch.Tensor(s.shape).fill_(self.hsv[1]).to(input.device)\n","        target_v = torch.Tensor(v.shape).fill_(self.hsv[2]).to(input.device)\n","        return self.mse(h, target_h) \n","\n","def InfiniteSampler(n):\n","    i = n - 1\n","    order = np.random.permutation(n)\n","    while True:\n","        yield order[i]\n","        i += 1\n","        if i >= n:\n","            np.random.seed()\n","            order = np.random.permutation(n)\n","            i = 0\n","\n","class InfiniteSamplerWrapper(data.sampler.Sampler):\n","    def __init__(self, data_source):\n","        self.num_samples = len(data_source)\n","\n","    def __iter__(self):\n","        return iter(InfiniteSampler(self.num_samples))\n","\n","    def __len__(self):\n","        return 2 ** 31\n","\n","\n","def _rescale(img):\n","    return img * 2.0 - 1.0\n","\n","def trans_maker(size=256):\n","\ttrans = transforms.Compose([ \n","\t\t\t\t\ttransforms.Resize((size+36, size+36)),\n","\t\t\t\t\ttransforms.RandomHorizontalFlip(),\n","\t\t\t\t\ttransforms.RandomCrop((size, size)),\n","\t\t\t\t\ttransforms.ToTensor(),\n","\t\t\t\t\t_rescale\n","\t\t\t\t\t])\n","\treturn trans\n","\n","def trans_maker_testing(size=256):\n","\ttrans = transforms.Compose([ \n","\t\t\t\t\ttransforms.Resize((size, size)),\n","\t\t\t\t\ttransforms.ToTensor(),\n","\t\t\t\t\t_rescale\n","\t\t\t\t\t])\n","\treturn trans\n","transform_gan = trans_maker(size=128)\n","\n","import torchvision.utils as vutils\n","import logging\n","logger = logging.getLogger(__name__)\n","\n","\n","def save_image(net, dataloader_A, device, cur_iter, trial, save_path):\n","    logger.info('Saving gan epoch {} images: {}'.format(cur_iter, save_path))\n","    net.eval()\n","    for p in net.parameters():\n","        data_type = p.type()\n","        break\n","    with torch.no_grad():\n","        for itx, data in enumerate(dataloader_A):\n","            g_img = net.gen_a2b(data[0].to(device).type(data_type))\n","            for i in range(g_img.size(0)):\n","                vutils.save_image(\n","                    g_img.cpu().float().add_(1).mul_(0.5),\n","                    os.path.join(save_path, \"{}_gan_epoch_{}_iter_{}_{}.jpg\".format(trial, cur_iter, itx, i)),)\n","    \n","    net.train()\n","    return save_path\n","\n","def save_model(net, save_folder, cuda_device, if_multi_gpu, trial, cur_iter):\n","    save_name = \"{}_gan_epoch_{}.pth\".format(trial, cur_iter)\n","    save_path = os.path.join(save_folder, save_name)\n","    logger.info('Saving gan model: {}'.format(save_path))\n","\n","    net.save(save_path)\n","\n","    for fname in os.listdir(save_folder):\n","        if fname.endswith('.pth') and fname != save_name:\n","            delete_path = os.path.join(save_folder, fname)\n","            os.remove(delete_path)\n","            logger.info('Deleted previous gan model: {}'.format(delete_path))\n","\n","    return save_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tS4NP7EeZgft"},"source":["torch.backends.cudnn.benchmark = True\n","\n","def creat_folder(save_folder, trial_name):\n","    saved_model_folder = os.path.join(save_folder, 'train_results/%s/models'%trial_name)\n","    saved_image_folder = os.path.join(save_folder, 'train_results/%s/images'%trial_name)\n","    folders = [os.path.join(save_folder, 'train_results'), os.path.join(save_folder, 'train_results/%s'%trial_name), \n","    os.path.join(save_folder, 'train_results/%s/images'%trial_name), os.path.join(save_folder, 'train_results/%s/models'%trial_name)]\n","\n","    for folder in folders:\n","        if not os.path.exists(folder):\n","            os.mkdir(folder)\n","    return saved_model_folder, saved_image_folder\n","\n","def train_d(net, data, label=\"real\"):\n","    pred = net(data)\n","    if label==\"real\":\n","        err = F.relu(1-pred).mean()\n","    else:\n","        err = F.relu(1+pred).mean()\n","\n","    err.backward()\n","    return torch.sigmoid(pred).mean().item()\n","\n","def gram_matrix(input):\n","    a, b, c, d = input.size()  \n","    features = input.view(a * b, c * d)  \n","    G = torch.mm(features, features.t())  \n","    return G.div(a * b * c * d)\n","\n","def gram_loss(input, target):\n","    in_gram = gram_matrix(input)\n","    tar_gram = gram_matrix(target.detach())\n","    return F.mse_loss(in_gram, tar_gram)\n","\n","def save_image(net_g, dataloader, saved_image_folder, n_iter, vgg, device, base):\n","    net_g.eval()\n","    with torch.no_grad():\n","        imgs = []\n","        real = []\n","        for i, d in enumerate(dataloader):\n","            if i < 2:\n","                f_3 = vgg(d[0].to(device), base=base)[2]\n","                imgs.append(net_g(f_3).cpu())\n","                real.append(d[0])\n","            else:\n","                break\n","        imgs = torch.cat(imgs, dim=0)\n","        real = torch.cat(real, dim=0)\n","        sss = torch.cat([imgs, real], dim=0)\n","        \n","        vutils.save_image( sss, \"%s/iter_%d.jpg\"%(saved_image_folder, n_iter), range=(-1,1), normalize=True)\n","        del imgs\n","    net_g.train()\n","\n","def train(net_g, net_d_style, max_iteration, save_folder, trial_name, dataloader_A_fixed, dataloader_B, dataloader_A, base, vgg, device, gram_reshape, optDS, optG):\n","    print('training begin ... ')\n","    titles = ['D_r', 'D_f', 'G', 'G_rec']\n","    losses = {title: 0.0 for title in titles}\n","    mse_weight = 0.2\n","    gram_weight = 1\n","    log_interval = 100\n","\n","    saved_model_folder, saved_image_folder = creat_folder(save_folder, trial_name)\n","    \n","    for n_iter in tqdm.tqdm(range(max_iteration+1)):\n","        if (n_iter+1)%(100)==0:\n","            try:\n","                model_dict = {'g': net_g.state_dict(), 'ds':net_d_style.state_dict()}\n","                torch.save(model_dict, os.path.join(saved_model_folder, '%d_model.pth'%(n_iter)))\n","                opt_dict = {'g': optG.state_dict(), 'ds':optDS.state_dict()}\n","                torch.save(opt_dict, os.path.join(saved_model_folder, '%d_opt.pth'%(n_iter)))\n","            except:\n","                print(\"models not properly saved\")\n","        if n_iter%100==0:\n","            save_image(net_g, dataloader_A_fixed, saved_image_folder, n_iter, vgg, device, base)\n","        \n","        \n","        real_style = next(dataloader_B)[0].to(device)\n","        real_content = next(dataloader_A)[0].to(device)\n","     \n","        cf_1, cf_2, cf_3, cf_4, cf_5 = vgg(real_content, base=base)\n","        sf_1, sf_2, sf_3, sf_4, sf_5 = vgg(real_style, base=base)\n","\n","        fake_img = net_g(cf_3)\n","        tf_1, tf_2, tf_3, tf_4, tf_5 = vgg(fake_img, base=base)\n","\n","        target_3 = adain(cf_3, sf_3)\n","\n","        gram_sf_4 = gram_reshape(get_batched_gram_matrix(sf_4))\n","        gram_sf_3 = gram_reshape(get_batched_gram_matrix(sf_3))\n","        gram_sf_2 = gram_reshape(get_batched_gram_matrix(sf_2))\n","        real_style_sample = torch.cat([gram_sf_2, gram_sf_3, gram_sf_4], dim=1)\n","\n","        gram_tf_4 = gram_reshape(get_batched_gram_matrix(tf_4))\n","        gram_tf_3 = gram_reshape(get_batched_gram_matrix(tf_3))\n","        gram_tf_2 = gram_reshape(get_batched_gram_matrix(tf_2))\n","        fake_style_sample = torch.cat([gram_tf_2, gram_tf_3, gram_tf_4], dim=1)\n","\n","        \n","        net_d_style.zero_grad()\n","        D_R = train_d(net_d_style, real_style_sample, label=\"real\")\n","        D_F = train_d(net_d_style, fake_style_sample.detach(), label=\"fake\")\n","        optDS.step()\n","        \n","        net_g.zero_grad()\n","        pred_gs = net_d_style(fake_style_sample)\n","        err_gs = -pred_gs.mean()\n","   \n","        G_B = torch.sigmoid(pred_gs).mean().item() \n","\n","        err_rec = F.mse_loss(tf_3, target_3)\n","        err_gram = 2000*(\n","            gram_loss(tf_4, sf_4) + \\\n","                gram_loss(tf_3, sf_3) + \\\n","                    gram_loss(tf_2, sf_2))\n","\n","        G_rec = err_gram.item()\n","\n","        err = err_gs + mse_weight*err_rec + gram_weight*err_gram\n","        err.backward()\n","\n","        optG.step()\n","        loss_values = [D_R, D_F, G_B, G_rec]\n","        for i, term in enumerate(titles):\n","            losses[term] += loss_values[i]\n","        \n","        if n_iter > 0 and n_iter % log_interval == 0:\n","            log_line = \"\"\n","            for key, value in losses.items():\n","                log_line += \"%s: %.5f  \"%(key, value/log_interval)\n","                losses[key] = 0\n","            print(log_line)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbysAs67ZgdE"},"source":["def executing(path_a, path_b):\n","\n","    checkpt = 'None'\n","    trial_name = \"test1\"\n","    data_root_A = path_a\n","    data_root_B = path_b\n","    mse_weight = 0.2\n","    gram_weight = 1\n","    max_iteration = 1000\n","    device = torch.device(\"cuda:%d\"%(0))\n","    batch_size = 4\n","    lr_ = 2e-4\n","\n","    im_size = 256\n","    if im_size == 128:\n","        base = 4\n","    elif im_size == 256:\n","        base = 8\n","    elif im_size == 512:\n","        base = 16\n","    if im_size not in [128, 256, 512]:\n","        print(\"the size must be in [128, 256, 512]\")\n","  \n","\n","    log_interval = 100\n","    save_folder = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/saved/'\n","    number_model_to_save = 30\n","\n","    vgg = VGGSimple()\n","    vgg.load_state_dict(torch.load('/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/vgg-feature-weights.pth', map_location=lambda a,b:a))\n","    vgg.to(device)\n","    vgg.eval()\n","    for p in vgg.parameters():\n","        p.requires_grad = False\n","\n","    dataset_A = Dataset.ImageFolder(root=data_root_A, transform=trans_maker(im_size)) \n","    dataloader_A_fixed = DataLoader(dataset_A, 8, shuffle=False, num_workers=4)\n","    dataloader_A = iter(DataLoader(dataset_A, batch_size, shuffle=False,\\\n","            sampler=InfiniteSamplerWrapper(dataset_A), num_workers=4, pin_memory=True))\n","\n","    dataset_B = Dataset.ImageFolder(root=data_root_B, transform=trans_maker(im_size)) \n","    dataloader_B = iter(DataLoader(dataset_B, batch_size, shuffle=False,\\\n","            sampler=InfiniteSamplerWrapper(dataset_B), num_workers=4, pin_memory=True))\n","\n","    net_g = Generator(infc=256, nfc=128)\n","\n","    net_d_style = Discriminator(nfc=128*3, norm_layer=nn.BatchNorm2d)\n","    gram_reshape = Adaptive_pool(128, 16)\n","    \n","    if checkpt is not 'None':\n","        checkpoint = torch.load(checkpt, map_location=lambda storage, loc: storage)\n","        net_g.load_state_dict(checkpoint['g'])\n","        net_d_style.load_state_dict(checkpoint['ds'])\n","        print(\"saved model loaded\")\n","\n","    net_d_style.to(device)\n","    net_g.to(device)   \n","\n","    optG = optim.Adam(net_g.parameters(), lr=lr_, betas=(0.5, 0.99))\n","    optDS = optim.Adam(net_d_style.parameters(), lr=lr_, betas=(0.5, 0.99))\n","    \n","    if checkpt is not 'None':\n","        opt_path = checkpt.replace(\"_model.pth\", \"_opt.pth\")\n","        try:\n","            opt_weights = torch.load(opt_path, map_location=lambda a, b: a)\n","            optG.load_state_dict(opt_weights['g'])\n","            optDS.load_state_dict(opt_weights['ds'])\n","            print(\"saved optimizer loaded\")\n","        except:\n","            print(\"no optimizer weights detected, resuming a training without optimizer weights may not let the model converge as desired\")\n","            pass\n","    \n","    train(net_g, net_d_style, max_iteration, save_folder, trial_name, dataloader_A_fixed, dataloader_B, dataloader_A, base, vgg, device, gram_reshape, optDS, optG)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"si5UIMg8hu_M"},"source":["## training for sketch synthesis"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyzvIRnDiENw","executionInfo":{"status":"ok","timestamp":1631504246360,"user_tz":-330,"elapsed":1557092,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00068853847012010225"}},"outputId":"ed33b557-7853-4a1f-8e6d-3c19a9452dd3"},"source":["img_path = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/images/'\n","skc_path = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/sketch/'\n","\n","executing(img_path, skc_path)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["training begin ... \n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1001 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n","/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n","  warnings.warn(warning)\n"," 10%|█         | 101/1001 [03:05<35:21,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.89513  D_f: 0.11602  G: 0.10068  G_rec: 1.81017  \n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 201/1001 [05:35<28:44,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.87728  D_f: 0.12672  G: 0.11242  G_rec: 0.90363  \n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 301/1001 [08:05<25:15,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.86820  D_f: 0.12276  G: 0.11374  G_rec: 0.75287  \n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 401/1001 [10:34<21:38,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.86533  D_f: 0.13738  G: 0.12805  G_rec: 0.65702  \n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 501/1001 [13:05<17:58,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.82999  D_f: 0.16613  G: 0.14405  G_rec: 0.55781  \n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 601/1001 [15:35<14:41,  2.20s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.84595  D_f: 0.15380  G: 0.12750  G_rec: 0.60710  \n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 701/1001 [18:05<10:48,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.83098  D_f: 0.16384  G: 0.13540  G_rec: 0.56120  \n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 801/1001 [20:35<07:12,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.82504  D_f: 0.15991  G: 0.13128  G_rec: 0.55975  \n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 901/1001 [23:05<03:36,  2.16s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.81668  D_f: 0.17630  G: 0.14758  G_rec: 0.52363  \n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1001/1001 [25:35<00:00,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["D_r: 0.83098  D_f: 0.16481  G: 0.13122  G_rec: 0.60554  \n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","metadata":{"id":"DVRVTAZfZgae"},"source":["import os\n","import torch\n","import torchvision.datasets as Dataset\n","import torchvision.utils as vutils\n","from torch import nn\n","import argparse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvFqbh6YZgX0"},"source":["def synthesizing(path_content, path_result, checkpoint):\n","\n","    device = torch.device(\"cuda:%d\"%(0))\n","\n","    im_size = 256\n","    if im_size == 128:\n","        base = 4\n","    elif im_size == 256:\n","        base = 8\n","    elif im_size == 512:\n","        base = 16\n","    elif im_size == 1024:\n","        base = 32\n","    if im_size not in [128, 256, 512, 1024]:\n","        print(\"the size must be in [128, 256, 512, 1024]\")\n","  \n","    vgg = VGGSimple()\n","    vgg.load_state_dict(torch.load('/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/vgg-feature-weights.pth', map_location=lambda a,b:a))\n","    vgg.to(device)\n","    vgg.eval()\n","    for p in vgg.parameters():\n","        p.requires_grad = False\n","\n","    dataset = Dataset.ImageFolder(root=path_content, transform=trans_maker_testing(size=im_size)) \n","    \n","    net_g = Generator(infc=256, nfc=128)\n","    \n","    if checkpoint is not 'None':\n","        checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n","        net_g.load_state_dict(checkpoint['g'])\n","        print(\"saved model loaded\")\n","\n","    net_g.to(device)   \n","    net_g.eval()\n","\n","    dist_path = path_result\n","    if not os.path.exists(dist_path):\n","        os.mkdir(dist_path)\n","\n","\n","    print(\"begin generating images ...\")\n","    with torch.no_grad():\n","        for i in range(len(dataset)):\n","            print(\"generating the %dth image\"%(i))\n","            img = dataset[i][0].to(device)\n","            feat = vgg(img, base=base)[2]\n","            g_img = net_g(feat)\n","\n","            g_img = g_img.mean(1).unsqueeze(1).detach().add(1).mul(0.5)\n","            g_img = (g_img > 0.7).float()\n","            vutils.save_image(g_img, os.path.join(dist_path, '%d.jpg'%(i)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRAcYGwxi5pn"},"source":["## sketch generation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkE1-bidx3WZ","executionInfo":{"status":"ok","timestamp":1633881739608,"user_tz":-330,"elapsed":346335,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"1c7f2386-2f82-4c36-9fa6-37c86d68b209"},"source":["path_content = \"/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/images/\"\n","path_result = \"/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/saved/train_results/synthesized_sketches\"\n","checkpoint = \"/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/saved/train_results/test1/models/899_model.pth\"\n","synthesizing(path_content, path_result, checkpoint)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["saved model loaded\n","begin generating images ...\n","generating the 0th image\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["generating the 1th image\n","generating the 2th image\n","generating the 3th image\n","generating the 4th image\n","generating the 5th image\n","generating the 6th image\n","generating the 7th image\n","generating the 8th image\n","generating the 9th image\n","generating the 10th image\n","generating the 11th image\n","generating the 12th image\n","generating the 13th image\n","generating the 14th image\n","generating the 15th image\n","generating the 16th image\n","generating the 17th image\n","generating the 18th image\n","generating the 19th image\n","generating the 20th image\n","generating the 21th image\n","generating the 22th image\n","generating the 23th image\n","generating the 24th image\n","generating the 25th image\n","generating the 26th image\n","generating the 27th image\n","generating the 28th image\n","generating the 29th image\n","generating the 30th image\n","generating the 31th image\n","generating the 32th image\n","generating the 33th image\n","generating the 34th image\n","generating the 35th image\n","generating the 36th image\n","generating the 37th image\n","generating the 38th image\n","generating the 39th image\n","generating the 40th image\n","generating the 41th image\n","generating the 42th image\n","generating the 43th image\n","generating the 44th image\n","generating the 45th image\n","generating the 46th image\n","generating the 47th image\n","generating the 48th image\n","generating the 49th image\n","generating the 50th image\n","generating the 51th image\n","generating the 52th image\n","generating the 53th image\n","generating the 54th image\n","generating the 55th image\n","generating the 56th image\n","generating the 57th image\n","generating the 58th image\n","generating the 59th image\n","generating the 60th image\n","generating the 61th image\n","generating the 62th image\n","generating the 63th image\n","generating the 64th image\n","generating the 65th image\n","generating the 66th image\n","generating the 67th image\n","generating the 68th image\n","generating the 69th image\n","generating the 70th image\n","generating the 71th image\n","generating the 72th image\n","generating the 73th image\n","generating the 74th image\n","generating the 75th image\n","generating the 76th image\n","generating the 77th image\n","generating the 78th image\n","generating the 79th image\n","generating the 80th image\n","generating the 81th image\n","generating the 82th image\n","generating the 83th image\n","generating the 84th image\n","generating the 85th image\n","generating the 86th image\n","generating the 87th image\n","generating the 88th image\n","generating the 89th image\n","generating the 90th image\n","generating the 91th image\n","generating the 92th image\n","generating the 93th image\n","generating the 94th image\n","generating the 95th image\n","generating the 96th image\n","generating the 97th image\n","generating the 98th image\n","generating the 99th image\n","generating the 100th image\n","generating the 101th image\n","generating the 102th image\n","generating the 103th image\n","generating the 104th image\n","generating the 105th image\n","generating the 106th image\n","generating the 107th image\n","generating the 108th image\n","generating the 109th image\n","generating the 110th image\n","generating the 111th image\n","generating the 112th image\n","generating the 113th image\n","generating the 114th image\n","generating the 115th image\n","generating the 116th image\n","generating the 117th image\n","generating the 118th image\n","generating the 119th image\n","generating the 120th image\n","generating the 121th image\n","generating the 122th image\n","generating the 123th image\n","generating the 124th image\n","generating the 125th image\n","generating the 126th image\n","generating the 127th image\n","generating the 128th image\n","generating the 129th image\n","generating the 130th image\n","generating the 131th image\n","generating the 132th image\n","generating the 133th image\n","generating the 134th image\n","generating the 135th image\n","generating the 136th image\n","generating the 137th image\n","generating the 138th image\n","generating the 139th image\n","generating the 140th image\n","generating the 141th image\n","generating the 142th image\n","generating the 143th image\n","generating the 144th image\n","generating the 145th image\n","generating the 146th image\n","generating the 147th image\n","generating the 148th image\n","generating the 149th image\n","generating the 150th image\n","generating the 151th image\n","generating the 152th image\n","generating the 153th image\n","generating the 154th image\n","generating the 155th image\n","generating the 156th image\n","generating the 157th image\n","generating the 158th image\n","generating the 159th image\n","generating the 160th image\n","generating the 161th image\n","generating the 162th image\n","generating the 163th image\n","generating the 164th image\n","generating the 165th image\n","generating the 166th image\n","generating the 167th image\n","generating the 168th image\n","generating the 169th image\n","generating the 170th image\n","generating the 171th image\n","generating the 172th image\n","generating the 173th image\n","generating the 174th image\n","generating the 175th image\n","generating the 176th image\n","generating the 177th image\n","generating the 178th image\n","generating the 179th image\n","generating the 180th image\n","generating the 181th image\n","generating the 182th image\n","generating the 183th image\n","generating the 184th image\n","generating the 185th image\n","generating the 186th image\n","generating the 187th image\n","generating the 188th image\n","generating the 189th image\n","generating the 190th image\n","generating the 191th image\n","generating the 192th image\n","generating the 193th image\n","generating the 194th image\n","generating the 195th image\n","generating the 196th image\n","generating the 197th image\n","generating the 198th image\n","generating the 199th image\n","generating the 200th image\n","generating the 201th image\n","generating the 202th image\n","generating the 203th image\n","generating the 204th image\n","generating the 205th image\n","generating the 206th image\n","generating the 207th image\n","generating the 208th image\n","generating the 209th image\n","generating the 210th image\n","generating the 211th image\n","generating the 212th image\n","generating the 213th image\n","generating the 214th image\n","generating the 215th image\n","generating the 216th image\n","generating the 217th image\n","generating the 218th image\n","generating the 219th image\n","generating the 220th image\n","generating the 221th image\n","generating the 222th image\n","generating the 223th image\n","generating the 224th image\n","generating the 225th image\n","generating the 226th image\n","generating the 227th image\n","generating the 228th image\n","generating the 229th image\n","generating the 230th image\n","generating the 231th image\n","generating the 232th image\n","generating the 233th image\n","generating the 234th image\n","generating the 235th image\n","generating the 236th image\n","generating the 237th image\n","generating the 238th image\n","generating the 239th image\n","generating the 240th image\n","generating the 241th image\n","generating the 242th image\n","generating the 243th image\n","generating the 244th image\n","generating the 245th image\n","generating the 246th image\n","generating the 247th image\n","generating the 248th image\n","generating the 249th image\n","generating the 250th image\n","generating the 251th image\n","generating the 252th image\n","generating the 253th image\n","generating the 254th image\n","generating the 255th image\n","generating the 256th image\n","generating the 257th image\n","generating the 258th image\n","generating the 259th image\n","generating the 260th image\n","generating the 261th image\n","generating the 262th image\n","generating the 263th image\n","generating the 264th image\n","generating the 265th image\n","generating the 266th image\n","generating the 267th image\n","generating the 268th image\n","generating the 269th image\n","generating the 270th image\n","generating the 271th image\n","generating the 272th image\n","generating the 273th image\n","generating the 274th image\n","generating the 275th image\n","generating the 276th image\n","generating the 277th image\n","generating the 278th image\n","generating the 279th image\n","generating the 280th image\n","generating the 281th image\n","generating the 282th image\n","generating the 283th image\n","generating the 284th image\n","generating the 285th image\n","generating the 286th image\n","generating the 287th image\n","generating the 288th image\n","generating the 289th image\n","generating the 290th image\n","generating the 291th image\n","generating the 292th image\n","generating the 293th image\n","generating the 294th image\n","generating the 295th image\n","generating the 296th image\n","generating the 297th image\n","generating the 298th image\n","generating the 299th image\n","generating the 300th image\n","generating the 301th image\n","generating the 302th image\n","generating the 303th image\n","generating the 304th image\n","generating the 305th image\n","generating the 306th image\n","generating the 307th image\n","generating the 308th image\n","generating the 309th image\n","generating the 310th image\n","generating the 311th image\n","generating the 312th image\n","generating the 313th image\n","generating the 314th image\n","generating the 315th image\n","generating the 316th image\n","generating the 317th image\n","generating the 318th image\n","generating the 319th image\n","generating the 320th image\n","generating the 321th image\n","generating the 322th image\n","generating the 323th image\n","generating the 324th image\n","generating the 325th image\n","generating the 326th image\n","generating the 327th image\n","generating the 328th image\n","generating the 329th image\n","generating the 330th image\n","generating the 331th image\n","generating the 332th image\n","generating the 333th image\n","generating the 334th image\n","generating the 335th image\n","generating the 336th image\n","generating the 337th image\n","generating the 338th image\n","generating the 339th image\n","generating the 340th image\n","generating the 341th image\n","generating the 342th image\n","generating the 343th image\n","generating the 344th image\n","generating the 345th image\n","generating the 346th image\n","generating the 347th image\n","generating the 348th image\n","generating the 349th image\n","generating the 350th image\n","generating the 351th image\n","generating the 352th image\n","generating the 353th image\n","generating the 354th image\n","generating the 355th image\n","generating the 356th image\n","generating the 357th image\n","generating the 358th image\n","generating the 359th image\n","generating the 360th image\n","generating the 361th image\n","generating the 362th image\n","generating the 363th image\n","generating the 364th image\n","generating the 365th image\n","generating the 366th image\n","generating the 367th image\n","generating the 368th image\n","generating the 369th image\n","generating the 370th image\n","generating the 371th image\n","generating the 372th image\n","generating the 373th image\n","generating the 374th image\n","generating the 375th image\n","generating the 376th image\n","generating the 377th image\n","generating the 378th image\n","generating the 379th image\n","generating the 380th image\n","generating the 381th image\n","generating the 382th image\n","generating the 383th image\n","generating the 384th image\n","generating the 385th image\n","generating the 386th image\n","generating the 387th image\n","generating the 388th image\n","generating the 389th image\n","generating the 390th image\n","generating the 391th image\n","generating the 392th image\n","generating the 393th image\n","generating the 394th image\n","generating the 395th image\n","generating the 396th image\n","generating the 397th image\n","generating the 398th image\n","generating the 399th image\n","generating the 400th image\n","generating the 401th image\n","generating the 402th image\n","generating the 403th image\n","generating the 404th image\n","generating the 405th image\n","generating the 406th image\n","generating the 407th image\n","generating the 408th image\n","generating the 409th image\n","generating the 410th image\n","generating the 411th image\n","generating the 412th image\n","generating the 413th image\n","generating the 414th image\n","generating the 415th image\n","generating the 416th image\n","generating the 417th image\n","generating the 418th image\n","generating the 419th image\n","generating the 420th image\n","generating the 421th image\n","generating the 422th image\n","generating the 423th image\n","generating the 424th image\n","generating the 425th image\n","generating the 426th image\n","generating the 427th image\n","generating the 428th image\n","generating the 429th image\n","generating the 430th image\n","generating the 431th image\n","generating the 432th image\n","generating the 433th image\n","generating the 434th image\n","generating the 435th image\n","generating the 436th image\n","generating the 437th image\n","generating the 438th image\n","generating the 439th image\n","generating the 440th image\n","generating the 441th image\n","generating the 442th image\n","generating the 443th image\n","generating the 444th image\n","generating the 445th image\n","generating the 446th image\n","generating the 447th image\n","generating the 448th image\n","generating the 449th image\n","generating the 450th image\n","generating the 451th image\n","generating the 452th image\n","generating the 453th image\n","generating the 454th image\n","generating the 455th image\n","generating the 456th image\n","generating the 457th image\n","generating the 458th image\n","generating the 459th image\n","generating the 460th image\n","generating the 461th image\n","generating the 462th image\n","generating the 463th image\n","generating the 464th image\n","generating the 465th image\n","generating the 466th image\n","generating the 467th image\n","generating the 468th image\n","generating the 469th image\n","generating the 470th image\n","generating the 471th image\n","generating the 472th image\n","generating the 473th image\n","generating the 474th image\n","generating the 475th image\n","generating the 476th image\n","generating the 477th image\n","generating the 478th image\n","generating the 479th image\n","generating the 480th image\n","generating the 481th image\n","generating the 482th image\n","generating the 483th image\n","generating the 484th image\n","generating the 485th image\n","generating the 486th image\n","generating the 487th image\n","generating the 488th image\n","generating the 489th image\n","generating the 490th image\n","generating the 491th image\n","generating the 492th image\n","generating the 493th image\n","generating the 494th image\n","generating the 495th image\n","generating the 496th image\n","generating the 497th image\n","generating the 498th image\n","generating the 499th image\n","generating the 500th image\n","generating the 501th image\n","generating the 502th image\n","generating the 503th image\n","generating the 504th image\n","generating the 505th image\n","generating the 506th image\n","generating the 507th image\n","generating the 508th image\n","generating the 509th image\n","generating the 510th image\n","generating the 511th image\n","generating the 512th image\n","generating the 513th image\n","generating the 514th image\n","generating the 515th image\n","generating the 516th image\n","generating the 517th image\n","generating the 518th image\n","generating the 519th image\n","generating the 520th image\n","generating the 521th image\n","generating the 522th image\n","generating the 523th image\n","generating the 524th image\n","generating the 525th image\n","generating the 526th image\n","generating the 527th image\n","generating the 528th image\n","generating the 529th image\n","generating the 530th image\n","generating the 531th image\n","generating the 532th image\n","generating the 533th image\n","generating the 534th image\n","generating the 535th image\n","generating the 536th image\n","generating the 537th image\n","generating the 538th image\n","generating the 539th image\n","generating the 540th image\n","generating the 541th image\n","generating the 542th image\n","generating the 543th image\n","generating the 544th image\n","generating the 545th image\n","generating the 546th image\n","generating the 547th image\n","generating the 548th image\n","generating the 549th image\n","generating the 550th image\n","generating the 551th image\n","generating the 552th image\n","generating the 553th image\n","generating the 554th image\n","generating the 555th image\n","generating the 556th image\n","generating the 557th image\n","generating the 558th image\n","generating the 559th image\n","generating the 560th image\n","generating the 561th image\n","generating the 562th image\n","generating the 563th image\n","generating the 564th image\n","generating the 565th image\n","generating the 566th image\n","generating the 567th image\n","generating the 568th image\n","generating the 569th image\n","generating the 570th image\n","generating the 571th image\n","generating the 572th image\n","generating the 573th image\n","generating the 574th image\n","generating the 575th image\n","generating the 576th image\n","generating the 577th image\n","generating the 578th image\n","generating the 579th image\n","generating the 580th image\n","generating the 581th image\n","generating the 582th image\n","generating the 583th image\n","generating the 584th image\n","generating the 585th image\n","generating the 586th image\n","generating the 587th image\n","generating the 588th image\n","generating the 589th image\n","generating the 590th image\n","generating the 591th image\n","generating the 592th image\n","generating the 593th image\n","generating the 594th image\n","generating the 595th image\n","generating the 596th image\n","generating the 597th image\n","generating the 598th image\n","generating the 599th image\n","generating the 600th image\n","generating the 601th image\n","generating the 602th image\n","generating the 603th image\n","generating the 604th image\n","generating the 605th image\n","generating the 606th image\n","generating the 607th image\n","generating the 608th image\n","generating the 609th image\n","generating the 610th image\n","generating the 611th image\n","generating the 612th image\n","generating the 613th image\n","generating the 614th image\n","generating the 615th image\n","generating the 616th image\n","generating the 617th image\n","generating the 618th image\n","generating the 619th image\n","generating the 620th image\n"]}]},{"cell_type":"code","metadata":{"id":"QpY3rJIIx3Ha","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633931728530,"user_tz":-330,"elapsed":2827,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"08ea907c-0d09-4422-ee97-de798e89dc93"},"source":["path_content = \"/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Fun/Img\"\n","path_result = \"/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Fun/Sketch\"\n","checkpoint = \"/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/saved/train_results/test1/models/899_model.pth\"\n","synthesizing(path_content, path_result, checkpoint)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["saved model loaded\n","begin generating images ...\n","generating the 0th image\n","generating the 1th image\n","generating the 2th image\n","generating the 3th image\n","generating the 4th image\n","generating the 5th image\n","generating the 6th image\n","generating the 7th image\n","generating the 8th image\n","generating the 9th image\n"]}]},{"cell_type":"code","metadata":{"id":"YCfiIuu7x3Eh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5mekyqMTEW8X"},"source":["import os\n","import torch\n","from copy import deepcopy\n","from random import shuffle\n","import torch.nn.functional as F\n","\n","def d_hinge_loss(real_pred, fake_pred):\n","    real_loss = F.relu(1-real_pred)\n","    fake_loss = F.relu(1+fake_pred)\n","\n","    return real_loss.mean() + fake_loss.mean()\n","\n","\n","def g_hinge_loss(pred):\n","    return -pred.mean()\n","\n","\n","class AverageMeter(object):\n","\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def true_randperm(size, device='cuda'):\n","    def unmatched_randperm(size):\n","        l1 = [i for i in range(size)]\n","        l2 = []\n","        for j in range(size):\n","            deleted = False\n","            if j in l1:\n","                deleted = True\n","                del l1[l1.index(j)]\n","            shuffle(l1)\n","            if len(l1) == 0:\n","                return 0, False\n","            l2.append(l1[0])\n","            del l1[0]\n","            if deleted:\n","                l1.append(j)\n","        return l2, True\n","    flag = False\n","    l = torch.zeros(size).long()\n","    while not flag:\n","        l, flag = unmatched_randperm(size)\n","    return torch.LongTensor(l).to(device)\n","\n","\n","def copy_G_params(model):\n","    flatten = deepcopy(list(p.data for p in model.parameters()))\n","    return flatten\n","\n","\n","def load_params(model, new_param):\n","    for p, new_p in zip(model.parameters(), new_param):\n","        p.data.copy_(new_p)\n","\n","\n","def make_folders(save_folder, trial_name):\n","    saved_model_folder = os.path.join(save_folder, 'train_results/%s/models'%trial_name)\n","    saved_image_folder = os.path.join(save_folder, 'train_results/%s/images'%trial_name)\n","    folders = [os.path.join(save_folder, 'train_results'), \n","               os.path.join(save_folder, 'train_results/%s'%trial_name), \n","               os.path.join(save_folder, 'train_results/%s/images'%trial_name), \n","               os.path.join(save_folder, 'train_results/%s/models'%trial_name)]\n","    for folder in folders:\n","        if not os.path.exists(folder):\n","            os.mkdir(folder)\n","    \n","    from shutil import copy\n","    try:\n","        for f in os.listdir('.'):\n","            if '.py' in f:\n","                copy(f, os.path.join(save_folder, 'train_results/%s'%trial_name)+'/'+f)\n","    except:\n","        pass\n","    return saved_image_folder, saved_model_folder \n","\n","\n","\n","import cv2\n","import numpy as np\n","import math\n","\n","def warp(img, mag=10, freq=100):\n","    rows, cols = img.shape\n","\n","    img_output = np.zeros(img.shape, dtype=img.dtype)\n","\n","    for i in range(rows):\n","        for j in range(cols):\n","            offset_x = int(mag * math.sin(2 * 3.14 * i / freq))\n","            offset_y = int(mag * math.cos(2 * 3.14 * j / freq))\n","            if i+offset_y < rows and j+offset_x < cols:\n","                img_output[i,j] = img[(i+offset_y)%rows,(j+offset_x)%cols]\n","            else:\n","                img_output[i,j] = 0\n","\n","    return img_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5853BFNFdq0"},"source":["import datetime\n","\n","DATA_NAME = 'face'\n","\n","DATALOADER_WORKERS = 4\n","NBR_CLS = 25\n","\n","EPOCH_GAN = 20\n","\n","SAVE_IMAGE_INTERVAL = 10\n","SAVE_MODEL_INTERVAL = 25\n","LOG_INTERVAL = 10\n","FID_INTERVAL = 25\n","FID_BATCH_NBR = 10\n","\n","ITERATION_AE = 250\n","\n","NFC=32\n","MULTI_GPU = False\n","\n","\n","IM_SIZE_GAN = 1024\n","BATCH_SIZE_GAN = 4\n","\n","IM_SIZE_AE = 512\n","BATCH_SIZE_AE = 8\n","\n","ct = datetime.datetime.now()  \n","TRIAL_NAME = 'trial-pr-%s-%d-%d-%d-%d'%(DATA_NAME, ct.month, ct.day, ct.hour, ct.minute)\n","SAVE_FOLDER = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/saved'\n","\n","PRETRAINED_AE_PATH = None \n","PRETRAINED_AE_ITER = 12000\n","\n","GAN_CKECKPOINT =None\n","\n","TRAIN_AE_ONLY = False\n","TRAIN_GAN_ONLY = False\n","\n","data_root_colorful = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/images/rgb_images'\n","data_root_sketch_1 = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/sketch/sketch-1'\n","data_root_sketch_2 = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/sketch/sketch-2'\n","data_root_sketch_3 = '/content/My_drive/MyDrive/rajat-Inspiron-3576/SEM7/DEEP_LEARNING/Project/Files/saved/train_results/synthesized_sketches'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVwQDfrVIMxu"},"source":["import torchvision.transforms.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kw1ttNW3IP6w","executionInfo":{"status":"ok","timestamp":1634052712183,"user_tz":-330,"elapsed":407,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"418bdca5-e5a3-43e1-8cde-9eabca7af7e0"},"source":["F.crop()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<module 'torchvision.transforms.functional' from '/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py'>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"N--FA-phExef"},"source":["import os\n","import random\n","import numpy as np\n","from PIL import Image, ImageFilter\n","from PIL import ImageFile\n","ImageFile.LOAD_TRUNCATED_IMAGES = True\n","\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","import torchvision.transforms.functional as Fff\n","import torch.utils.data as data\n","\n","normalize_imagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                     std=[0.229, 0.224, 0.225])\n","\n","def _noise_adder(img):\n","    return torch.empty_like(img, dtype=img.dtype).uniform_(0.0, 1/128.0) + img\n","\n","\n","def _rescale(img):\n","    return img * 2.0 - 1.0\n","\n","\n","def trans_maker(size=512):\n","    trans = transforms.Compose([\n","                    transforms.Resize((size, size)),\n","                    transforms.ToTensor(),\n","                    _rescale\n","                    ])\n","    return trans\n","\n","\n","def trans_maker_augment(size=256):\n","    trans = transforms.Compose([ \n","                    transforms.Resize((int(size*1.1),int(size*1.1))),\n","                    transforms.RandomHorizontalFlip(),\n","                    transforms.RandomCrop((size, size)),\n","                    transforms.ToTensor(),\n","                    _rescale\n","                    ])\n","    return trans\n","\n","\n","class SelfSupervisedDataset(Dataset):\n","    def __init__(self, data_root, data_root_2, im_size=512, nbr_cls=2000, rand_crop=True):\n","        super(SelfSupervisedDataset, self).__init__()\n","        self.root = data_root\n","        self.skt_root = data_root_2\n","\n","        self.frame = self._parse_frame()\n","        random.shuffle(self.frame)\n","\n","        self.nbr_cls = nbr_cls\n","        self.set_offset = 0\n","\n","        self.im_size = im_size\n","        self.transform_rd = transforms.Compose([ \n","                            transforms.Resize((int(im_size*1.3), int(im_size*1.3))),\n","                            transforms.RandomCrop( (int(im_size), int(im_size)) ),\n","                            transforms.RandomRotation( 30 ),\n","                            transforms.RandomHorizontalFlip(),\n","                            transforms.ToTensor(),\n","                            _rescale])\n","\n","        self.crop = rand_crop\n","        if self.crop:\n","            self.transform_1 = transforms.Resize((int(im_size*1.1), int(im_size*1.1)))\n","            self.transform_2 = transforms.Compose([ transforms.ToTensor(),\n","                                                    _rescale\n","                                                    ])\n","            self.rand_range = int(self.im_size * 0.1)\n","        else:\n","            self.transform_normal = trans_maker(size=im_size)\n","    \n","        self.transform_flip = transforms.RandomHorizontalFlip(p=1)\n","\n","        self.transform_erase = transforms.Compose([\n","                        transforms.RandomErasing(p=0.8, scale=(0.02, 0.1), value=1),\n","                        transforms.RandomErasing(p=0.8, scale=(0.02, 0.1), value=1),\n","                                ])\n","\n","    def _parse_frame(self):\n","        frame = []\n","        img_names = os.listdir(self.root)\n","        img_names.sort()\n","        for i in range(len(img_names)):\n","            img_name = '%d.jpg'%(i)\n","            img_name = img_names[i]\n","            image_path = os.path.join(self.root, img_names[i])\n","            skt_path = os.path.join( self.skt_root,  img_name)\n","            if os.path.exists(image_path) and os.path.exists(skt_path): \n","                frame.append( (image_path, skt_path) )\n","        return frame\n","\n","    def __len__(self):\n","        return self.nbr_cls\n","\n","    def _next_set(self):\n","        self.set_offset += self.nbr_cls\n","        if self.set_offset > ( len(self.frame) - self.nbr_cls ):\n","            random.shuffle(self.frame)\n","            self.set_offset = 0\n","\n","    def __getitem__(self, idx):\n","        file, skt_path = self.frame[idx+self.set_offset]\n","        img = Image.open(file).convert('RGB')\n","        skt = Image.open(skt_path).convert('L')\n","        \n","        bold_factor = 3 \n","        skt_bold = skt.filter( ImageFilter.MinFilter(size=bold_factor) )\n","\n","        if random.randint(0, 1) == 1:\n","            img = self.transform_flip(img)\n","            skt = self.transform_flip(skt)\n","            skt_bold = self.transform_flip(skt_bold)\n","\n","        img_rd = self.transform_rd(img) \n","\n","        if self.crop:\n","            img_normal = self.transform_1(img) \n","            skt_normal = self.transform_1(skt) \n","            skt_bold = self.transform_1(skt_bold) \n","\n","            i = random.randint(0, self.rand_range) \n","            j = random.randint(0, self.rand_range) \n","\n","            img_normal = Fff.crop(img_normal, i, j, self.im_size, self.im_size)\n","            skt_normal = Fff.crop(skt_normal, i, j, self.im_size, self.im_size)\n","            skt_bold = Fff.crop(skt_bold, i, j, self.im_size, self.im_size)\n","\n","            img_normal = self.transform_2(img_normal) \n","            skt_normal = self.transform_2(skt_normal) \n","            skt_bold = self.transform_2(skt_bold) \n","        else:\n","            img_normal = self.transform_normal(img)\n","            skt_normal = self.transform_normal(skt)\n","            skt_bold = self.transform_normal(skt_bold)\n","\n","        skt_erased = self.transform_erase(skt_normal)\n","        skt_erased_bold = self.transform_erase(skt_bold)\n","        return img_rd, img_normal, skt_normal, skt_bold, skt_erased, skt_erased_bold, idx\n","\n","\n","class PairedMultiDataset(Dataset):\n","    def __init__(self, data_root_1, data_root_2, data_root_3, data_root_4, rand_crop=True, im_size=512):\n","        super(PairedMultiDataset, self).__init__()\n","        self.root_a = data_root_1\n","        self.root_b = data_root_2\n","        self.root_c = data_root_3\n","        self.root_d = data_root_4\n","\n","        self.frame = self._parse_frame()\n","        \n","        self.crop = rand_crop\n","        self.im_size = im_size\n","        if self.crop:\n","            self.transform_1 = transforms.Resize((int(im_size*1.1), int(im_size*1.1)))\n","            self.transform_2 = transforms.Compose([ transforms.ToTensor(),\n","                                                    _rescale\n","                                                    ])\n","            self.rand_range = int(self.im_size * 0.1)\n","        else:\n","            self.transform = trans_maker( int( im_size ) )\n","\n","    def _parse_frame(self):\n","        frame = []\n","\n","        img_names = os.listdir(self.root_a)\n","        img_names.sort()\n","        for i in range(len(img_names)):\n","            img_name = '%d.jpg'%(i)\n","            img_name = img_names[i]\n","            image_a_path = os.path.join(self.root_a, img_names[i])\n","            if os.path.exists(image_a_path): \n","                image_b_path = os.path.join(self.root_b, img_name)\n","                image_c_path = os.path.join(self.root_c, img_name)\n","                image_d_path = os.path.join(self.root_d, img_name)\n","                if os.path.exists(image_b_path) and os.path.exists(image_c_path) and os.path.exists(image_d_path):\n","                    frame.append( (image_a_path, image_b_path, image_c_path, image_d_path) )\n","                else:\n","                    print('2', image_a_path, image_b_path)\n","            else:\n","                print(\"1\", image_a_path)\n","        return frame\n","\n","    def __len__(self):\n","        return len(self.frame)\n","\n","    def __getitem__(self, idx):\n","        file_a, file_b, file_c, file_d = self.frame[idx]\n","        img_a = Image.open(file_a).convert('RGB')\n","        img_b = Image.open(file_b).convert('L')\n","        img_c = Image.open(file_c).convert('L')\n","        img_d = Image.open(file_d).convert('L')\n","            \n","        if self.crop:\n","            img_a = self.transform_1(img_a) \n","            img_b = self.transform_1(img_b) \n","            img_c = self.transform_1(img_c) \n","            img_d = self.transform_1(img_d) \n","\n","            i = random.randint(0, self.rand_range) \n","            j = random.randint(0, self.rand_range) \n","            img_a = Fff.crop(img_a, i, j, self.im_size, self.im_size)\n","            img_b = Fff.crop(img_b, i, j, self.im_size, self.im_size)\n","            img_c = Fff.crop(img_c, i, j, self.im_size, self.im_size)\n","            img_d = Fff.crop(img_d, i, j, self.im_size, self.im_size)\n","\n","            img_a = self.transform_2(img_a) \n","            img_b = self.transform_2(img_b) \n","            img_c = self.transform_2(img_c) \n","            img_d = self.transform_2(img_d) \n","        else:\n","            img_a = self.transform(img_a) \n","            img_b = self.transform(img_b) \n","            img_c = self.transform(img_c) \n","            img_d = self.transform(img_d) \n","\n","\n","        return (img_a, img_b, img_c, img_d)\n","\n","\n","class PairedDataset(Dataset):\n","    def __init__(self, data_root_1, data_root_2, transform=trans_maker(512)):\n","        super(PairedDataset, self).__init__()\n","        self.root_a = data_root_1\n","        self.root_b = data_root_2\n","\n","        self.frame = self._parse_frame()\n","        self.transform = transform\n","\n","\n","    def _parse_frame(self):\n","        frame = []\n","        img_names = os.listdir(self.root_a)\n","        img_names.sort()\n","        for i in range(len(img_names)):\n","            img_name = '%d.jpg'%(i)\n","            if DATA_NAME == 'shoe':\n","                img_name = img_names[i]\n","            image_a_path = os.path.join(self.root_a, img_names[i])\n","            if ('.jpg' in image_a_path) or ('.png' in image_a_path): \n","                image_b_path = os.path.join(self.root_b, img_name)\n","                if os.path.exists(image_b_path):\n","                    frame.append( (image_a_path, image_b_path) )\n","\n","        return frame\n","\n","    def __len__(self):\n","        return len(self.frame)\n","\n","    def __getitem__(self, idx):\n","        file_a, file_b = self.frame[idx]\n","        img_a = Image.open(file_a).convert('RGB')\n","        img_b = Image.open(file_b).convert('L')\n","            \n","        if self.transform:\n","            img_a = self.transform(img_a) \n","            img_b = self.transform(img_b) \n","\n","        return (img_a, img_b)\n","\n","\n","class  ImageFolder(Dataset):\n","    def __init__(self, data_root, transform=trans_maker(512)):\n","        super( ImageFolder, self).__init__()\n","        self.root = data_root\n","\n","        self.frame = self._parse_frame()\n","        self.transform = transform\n","\n","    def _parse_frame(self):\n","        frame = []\n","        img_names = os.listdir(self.root)\n","        img_names.sort()\n","        for i in range(len(img_names)):\n","            image_path = os.path.join(self.root, img_names[i])\n","            if ('.jpg' in image_path) or ('.png' in image_path): \n","                frame.append(image_path)\n","\n","        return frame\n","\n","    def __len__(self):\n","        return len(self.frame)\n","\n","    def __getitem__(self, idx):\n","        file = self.frame[idx]\n","        img = Image.open(file).convert('RGB')\n","            \n","        if self.transform:\n","            img = self.transform(img) \n","        return img\n","\n","\n","\n","\n","def InfiniteSampler(n):\n","    i = n - 1\n","    order = np.random.permutation(n)\n","    while True:\n","        yield order[i]\n","        i += 1\n","        if i >= n:\n","            np.random.seed()\n","            order = np.random.permutation(n)\n","            i = 0\n","\n","\n","class InfiniteSamplerWrapper(data.sampler.Sampler):\n","    def __init__(self, data_source):\n","        self.num_samples = len(data_source)\n","\n","    def __iter__(self):\n","        return iter(InfiniteSampler(self.num_samples))\n","\n","    def __len__(self):\n","        return 2 ** 31"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmQQoG_sLsBB","executionInfo":{"status":"ok","timestamp":1634050546516,"user_tz":-330,"elapsed":8617,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"6e15cd06-ae9c-4ec0-ffcc-feb6c867a3cc"},"source":["!pip uninstall torch_dwconv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch-dwconv 0.1.0\n","Uninstalling torch-dwconv-0.1.0:\n","  Would remove:\n","    /usr/local/lib/python3.7/dist-packages/torch_dwconv-0.1.0.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/torch_dwconv/*\n","Proceed (y/n)? y\n","  Successfully uninstalled torch-dwconv-0.1.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWsQUH_DARN2","executionInfo":{"status":"ok","timestamp":1634107307585,"user_tz":-330,"elapsed":69148,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"8cf3ec8c-5a44-442d-e3e0-fea13bb0cb39"},"source":["!pip install torch_dwconv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch_dwconv\n","  Downloading torch_dwconv-0.1.0.tar.gz (249 kB)\n","\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 30 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 40 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 51 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 61 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 71 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 81 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 92 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 102 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 112 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 122 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 133 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 143 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 153 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 163 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 174 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 184 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 194 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 204 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 215 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 225 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 235 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 245 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 249 kB 12.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: torch-dwconv\n","  Building wheel for torch-dwconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for torch-dwconv: filename=torch_dwconv-0.1.0-cp37-cp37m-linux_x86_64.whl size=2644603 sha256=24db050c25406edb9b0c253cd768b61b2d1aa7c6071fbdf37fe3cbaff282e0fd\n","  Stored in directory: /root/.cache/pip/wheels/fc/ce/e6/33361c4cae3eb5d8649e1fce38efdb2f1013f01fa586b81f31\n","Successfully built torch-dwconv\n","Installing collected packages: torch-dwconv\n","Successfully installed torch-dwconv-0.1.0\n"]}]},{"cell_type":"code","metadata":{"id":"QLVVgMZIJiQx"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.utils import spectral_norm\n","from torch_dwconv import DepthwiseConv2d\n","\n","import math\n","import random\n","\n","\n","def weights_init(m):\n","    classname = m.__class__.__name__\n","    try:\n","        if classname.find('Conv') != -1:\n","            m.weight.data.normal_(0.0, 0.02)\n","        elif classname.find('BatchNorm') != -1:\n","            m.weight.data.normal_(1.0, 0.02)\n","            m.bias.data.fill_(0)\n","    except:\n","        pass\n","\n","\n","class DMI(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        \n","        self.weight_a = nn.Parameter(torch.ones(1, in_channels, 1, 1)*1.01)\n","        self.weight_b = nn.Parameter(torch.ones(1, in_channels, 1, 1)*0.99)\n","\n","        self.bias_a = nn.Parameter(torch.zeros(1, in_channels, 1, 1)+0.01)\n","        self.bias_b = nn.Parameter(torch.zeros(1, in_channels, 1, 1)-0.01)\n","\n","    def forward(self, feat, mask):\n","        if feat.shape[1] > mask.shape[1]:\n","            channel_scale = feat.shape[1] // mask.shape[1]\n","            mask = mask.repeat(1, channel_scale, 1, 1)\n","        \n","        mask = F.interpolate(mask, size=feat.shape[2])\n","        feat_a = self.weight_a * feat * mask + self.bias_a\n","        feat_b = self.weight_b * feat * (1-mask) + self.bias_b\n","        return feat_a + feat_b\n","\n","\n","class Swish(nn.Module):\n","    def forward(self, feat):\n","        return feat * torch.sigmoid(feat)\n","\n","\n","class Squeeze(nn.Module):\n","    def forward(self, feat):\n","        return feat.squeeze(-1).squeeze(-1)\n","\n","\n","class UnSqueeze(nn.Module):\n","    def forward(self, feat):\n","        return feat.unsqueeze(-1).unsqueeze(-1)\n","\n","\n","class GLU(nn.Module):\n","    def __init__(self):\n","        super(GLU, self).__init__()\n","\n","    def forward(self, x):\n","        nc = x.size(1)\n","        assert nc % 2 == 0, 'channels dont divide 2!'\n","        nc = int(nc/2)\n","        return x[:, :nc] * torch.sigmoid(x[:, nc:])\n","\n","\n","class NoiseInjection(nn.Module):\n","    def __init__(self, ch):\n","        super().__init__()\n","\n","        self.weight = nn.Parameter(torch.zeros(1, ch, 1, 1), requires_grad=True)\n","\n","    def forward(self, feat, noise=None):\n","        if noise is None:\n","            batch, _, height, width = feat.shape\n","            noise = torch.randn(batch, 1, height, width).to(feat.device)\n","\n","        return feat + self.weight * noise\n","\n","\n","class SELayer(nn.Module):\n","    def __init__(self, channel, reduction=16):\n","        super(SELayer, self).__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            spectral_norm( nn.Linear(channel, channel // reduction, bias=False) ),\n","            nn.ReLU(inplace=True),\n","            spectral_norm( nn.Linear(channel // reduction, channel, bias=False) ),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y.expand_as(x)\n","\n","\n","class ResBlkG(nn.Module):\n","    def __init__(self, ch, ch_m=4):\n","        super().__init__()\n","        self.main = nn.Sequential(spectral_norm( nn.BatchNorm2d(ch) ),\n","                            spectral_norm( nn.Conv2d(ch, ch*ch_m, 1, 1, 0, bias=False) ),\n","                            spectral_norm( nn.BatchNorm2d(ch*ch_m) ), Swish(),\n","                            spectral_norm( DepthwiseConv2d(ch*ch_m, ch*ch_m, 5, 1, 2) ),\n","                            spectral_norm( nn.BatchNorm2d(ch*ch_m) ), Swish(),\n","                            spectral_norm( nn.Conv2d(ch*ch_m, ch, 1, 1, 0, bias=False) ),\n","                            spectral_norm( nn.BatchNorm2d(ch) ),\n","                            SELayer(ch))\n","    def forward(self, feat):\n","        return feat + self.main(feat)\n","\n","\n","class ResBlkE(nn.Module):\n","    def __init__(self, ch):\n","        super().__init__()\n","\n","        self.main = nn.Sequential(\n","                            spectral_norm( nn.BatchNorm2d(ch) ), Swish(),\n","                            spectral_norm( nn.Conv2d(ch, ch, 3, 1, 1, bias=False) ),\n","                            spectral_norm( nn.BatchNorm2d(ch) ), Swish(),\n","                            spectral_norm( nn.Conv2d(ch, ch, 3, 1, 1, bias=False) ),\n","                            SELayer(ch))\n","\n","    def forward(self, feat):\n","        return feat + self.main(feat)\n","\n","\n","class StyleEncoder(nn.Module):\n","    def __init__(self, nfc=64, nbr_cls=500):\n","        super().__init__()\n","\n","        self.nfc = nfc\n","\n","        self.sf_256 = nn.Sequential(nn.Conv2d(3, nfc//4, 4, 2, 1, bias=False),nn.LeakyReLU(0.2,inplace=True))\n","        self.sf_128 = nn.Sequential(nn.Conv2d(nfc//4, nfc//2, 4, 2, 1, bias=False),nn.BatchNorm2d(nfc//2),nn.LeakyReLU(0.1,inplace=True)) \n","        self.sf_64 = nn.Sequential(nn.Conv2d(nfc//2, nfc, 4, 2, 1, bias=False),nn.BatchNorm2d(nfc),nn.LeakyReLU(0.1,inplace=True)) \n","        \n","        self.sf_32 = nn.Sequential(nn.Conv2d(nfc, nfc*2, 4, 2, 1, bias=False), ResBlkE(nfc*2))\n","        self.sf_16 = nn.Sequential(nn.LeakyReLU(0.1,inplace=True), nn.Conv2d(nfc*2, nfc*4, 4, 2, 1, bias=False), ResBlkE(nfc*4))\n","        self.sf_8 = nn.Sequential(nn.LeakyReLU(0.1,inplace=True), nn.Conv2d(nfc*4, nfc*8, 4, 2, 1, bias=False), ResBlkE(nfc*8))\n","        \n","        self.sfv_32 = nn.Sequential( nn.AdaptiveAvgPool2d(output_size=4), nn.Conv2d(nfc*2, nfc*2, 4, 1, 0, bias=False), Squeeze() )\n","        self.sfv_16 = nn.Sequential( nn.AdaptiveAvgPool2d(output_size=4), nn.Conv2d(nfc*4, nfc*4, 4, 1, 0, bias=False), Squeeze() )\n","        self.sfv_8 = nn.Sequential( nn.AdaptiveAvgPool2d(output_size=4), nn.Conv2d(nfc*8, nfc*8, 4, 1, 0, bias=False), Squeeze() )\n","\n","        self.nbr_cls = nbr_cls\n","        self.final_cls = None\n","\n","    def reset_cls(self):\n","        if self.final_cls is None:\n","            self.final_cls = nn.Sequential(nn.LeakyReLU(0.1), nn.Linear(self.nfc*8, self.nbr_cls))\n","        stdv = 1. / math.sqrt(self.final_cls[1].weight.size(1))\n","        self.final_cls[1].weight.data.uniform_(-stdv, stdv)\n","        if self.final_cls[1].bias is not None:\n","            self.final_cls[1].bias.data.uniform_(-0.1*stdv, 0.1*stdv)\n","\n","    def get_feats(self, image):\n","        feat = self.sf_256(image)\n","        feat = self.sf_128(feat)\n","        feat = self.sf_64(feat)\n","        feat_32 = self.sf_32(feat)\n","        feat_16 = self.sf_16(feat_32)\n","        feat_8 = self.sf_8(feat_16)\n","        \n","        feat_32 = self.sfv_32(feat_32)\n","        feat_16 = self.sfv_16(feat_16)\n","        feat_8 = self.sfv_8(feat_8)\n","\n","        return feat_32, feat_16, feat_8\n","\n","    def forward(self, image):\n","        feat_32, feat_16, feat_8 = self.get_feats(image)\n","\n","        pred_cls = self.final_cls(feat_8)\n","        return [feat_32, feat_16, feat_8], pred_cls\n","\n","\n","class ContentEncoder(nn.Module):\n","    def __init__(self, nfc=64):\n","        super().__init__()\n","\n","        self.cf_256 = nn.Sequential(nn.Conv2d(1, nfc//4, 4, 2, 1, bias=False),nn.LeakyReLU(0.2,inplace=True))\n","        self.cf_128 = nn.Sequential(nn.Conv2d(nfc//4, nfc//2, 4, 2, 1, bias=False),nn.BatchNorm2d( nfc//2),nn.LeakyReLU(0.1,inplace=True)) \n","        self.cf_64 = nn.Sequential(nn.Conv2d( nfc//2, nfc, 4, 2, 1, bias=False),nn.BatchNorm2d(nfc),nn.LeakyReLU(0.1,inplace=True)) \n","        \n","        self.cf_32 = nn.Sequential(nn.Conv2d(nfc, nfc*2, 4, 2, 1, bias=False), ResBlkE(nfc*2))\n","        self.cf_16 = nn.Sequential(nn.LeakyReLU(0.1,inplace=True), nn.Conv2d(nfc*2, nfc*4, 4, 2, 1, bias=False), ResBlkE(nfc*4))\n","        self.cf_8 = nn.Sequential(nn.LeakyReLU(0.1,inplace=True), nn.Conv2d(nfc*4, nfc*8, 4, 2, 1, bias=False), ResBlkE(nfc*8))\n","        \n","    def get_feats(self, image):\n","        feat = self.cf_256(image)\n","        feat = self.cf_128(feat)\n","        feat = self.cf_64(feat)\n","        feat_32 = self.cf_32(feat)\n","        feat_16 = self.cf_16(feat_32)\n","        feat_8 = self.cf_8(feat_16)\n","\n","        return feat_32, feat_16, feat_8\n","\n","    def forward(self, image):\n","        feat_32, feat_16, feat_8 = self.get_feats(image)\n","        return [feat_32, feat_16, feat_8]\n","\n","\n","def up_decoder(ch_in, ch_out):\n","    return nn.Sequential(\n","        nn.UpsamplingNearest2d(scale_factor=2),\n","        nn.Conv2d(ch_in, ch_out*2, 3, 1, 1, bias=False),\n","        nn.InstanceNorm2d( ch_out*2 ), GLU())\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, nfc=64):\n","        super().__init__()\n"," \n","        self.base_feat = nn.Parameter(torch.randn(1, nfc*8, 8, 8).normal_(0, 1), requires_grad=True)\n","        \n","        self.dmi_8 = DMI(nfc*8)\n","        self.dmi_16 = DMI(nfc*4)\n","\n","        self.feat_8_1 = nn.Sequential( ResBlkG(nfc*16), nn.LeakyReLU(0.1,inplace=True), nn.Conv2d(nfc*16, nfc*8, 3, 1, 1, bias=False), nn.InstanceNorm2d(nfc*8) )\n","        self.feat_8_2 = nn.Sequential( nn.LeakyReLU(0.1,inplace=True), ResBlkG(nfc*8) )\n","        self.feat_16  = nn.Sequential( nn.LeakyReLU(0.1,inplace=True), nn.UpsamplingNearest2d(scale_factor=2), nn.Conv2d(nfc*8, nfc*4, 3, 1, 1, bias=False), ResBlkG(nfc*4) )\n","        self.feat_32  = nn.Sequential( nn.LeakyReLU(0.1,inplace=True), nn.UpsamplingNearest2d(scale_factor=2), nn.Conv2d(nfc*8, nfc*2, 3, 1, 1, bias=False), ResBlkG(nfc*2) )\n","        self.feat_64  = nn.Sequential( nn.LeakyReLU(0.1,inplace=True), up_decoder(nfc*4, nfc) ) \n","        self.feat_128 = up_decoder(nfc*1, nfc//2)\n","        self.feat_256 = up_decoder(nfc//2, nfc//4)\n","        self.feat_512 = up_decoder(nfc//4, nfc//8)\n","        \n","        self.to_rgb = nn.Sequential( nn.Conv2d(nfc//8, 3, 3, 1, 1, bias=False), nn.Tanh() )\n","        \n","        self.style_8 = nn.Sequential( nn.Linear(nfc*8, nfc*8), nn.ReLU(), nn.Linear(nfc*8, nfc*8), nn.BatchNorm1d(nfc*8), UnSqueeze() )\n","        self.style_64 = nn.Sequential( nn.Linear(nfc*8, nfc), nn.ReLU(), nn.Linear(nfc, nfc), nn.Sigmoid() , UnSqueeze())\n","        self.style_128 = nn.Sequential( nn.Linear(nfc*4, nfc//2), nn.ReLU(), nn.Linear(nfc//2, nfc//2), nn.Sigmoid() , UnSqueeze())\n","        self.style_256 = nn.Sequential( nn.Linear(nfc*2, nfc//4), nn.ReLU(), nn.Linear(nfc//4, nfc//4), nn.Sigmoid() , UnSqueeze())\n","\n","    def forward(self, content_feats, style_vectors):\n","\n","        feat_8 = self.feat_8_1( torch.cat( [content_feats[2], self.base_feat.repeat(style_vectors[0].shape[0], 1, 1, 1)], dim=1 ) )            \n","        feat_8 = self.dmi_8(feat_8, content_feats[2])\n","\n","        bs = feat_8.shape[0]\n","\n","        feat_8 = feat_8 * self.style_8( style_vectors[2] )\n","        feat_8 = self.feat_8_2(feat_8)\n","\n","        feat_16 = self.feat_16(feat_8) \n","        feat_16 = self.dmi_16(feat_16, content_feats[1])\n","        feat_16 = torch.cat([feat_16, content_feats[1]], dim=1)\n","\n","        feat_32 = self.feat_32(feat_16) \n","        feat_32 = torch.cat([feat_32, content_feats[0]], dim=1)\n","\n","        feat_64 = self.feat_64(feat_32) * self.style_64(style_vectors[2]) \n","        feat_128 = self.feat_128(feat_64) * self.style_128(style_vectors[1]) \n","        feat_256 = self.feat_256(feat_128) * self.style_256(style_vectors[0]) \n","        feat_512 = self.feat_512(feat_256) \n","\n","        return self.to_rgb(feat_512)\n","\n","\n","class AE(nn.Module):\n","    def __init__(self, nfc, nbr_cls=500):\n","        super().__init__()  \n","\n","        self.style_encoder = StyleEncoder(nfc, nbr_cls=nbr_cls)\n","        self.content_encoder = ContentEncoder(nfc)\n","        self.decoder = Decoder(nfc)\n","\n","    @torch.no_grad()\n","    def forward(self, skt_img, style_img):\n","        style_feats = self.style_encoder.get_feats( F.interpolate(style_img, size=512) )\n","        content_feats = self.content_encoder( F.interpolate( skt_img , size=512) )\n","        gimg = self.decoder(content_feats, style_feats)\n","        return gimg, style_feats\n","\n","    def load_state_dicts(self, path):\n","        ckpt = torch.load(path)\n","        self.style_encoder.reset_cls()\n","        self.style_encoder.load_state_dict(ckpt['s'])\n","        self.content_encoder.load_state_dict(ckpt['c'])\n","        self.decoder.load_state_dict(ckpt['d'])\n","        print('AE load success')\n","\n","def down_gan(ch_in, ch_out):\n","    return nn.Sequential(\n","        spectral_norm(nn.Conv2d(ch_in, ch_out, 4, 2, 1, bias=False)),\n","        nn.BatchNorm2d(ch_out), nn.LeakyReLU(0.2, inplace=True))\n","\n","\n","def up_gan(ch_in, ch_out):\n","    return nn.Sequential(\n","        nn.UpsamplingNearest2d(scale_factor=2),\n","        spectral_norm( nn.Conv2d(ch_in, ch_out*2, 3, 1, 1, bias=False) ),\n","        nn.BatchNorm2d( ch_out*2 ), NoiseInjection(ch_out*2), GLU())\n","\n","\n","def repeat_upscale(feat, scale_factor=2):\n","    feat = feat.repeat(1,1,scale_factor,scale_factor)\n","    return feat\n","\n","\n","class RefineGenerator_art(nn.Module):\n","    def __init__(self, nfc=64, im_size=512):\n","        super().__init__()  \n","\n","        self.im_size = im_size\n","\n","        d16, d32, d64, d128, d256, d512 = nfc*8, nfc*8, nfc*4, nfc*2, nfc, nfc//2 \n","\n","        self.from_noise_32 = nn.Sequential( UnSqueeze(),\n","            spectral_norm(nn.ConvTranspose2d(nfc*8, nfc*8, 4, 1, 0, bias=False)), #4\n","            nn.BatchNorm2d(nfc*8), GLU(), up_gan(nfc*4, nfc*2),  up_gan(nfc*2, nfc*2), up_gan(nfc*2, nfc*1)) #32 \n","\n","        self.from_style = nn.Sequential( UnSqueeze(),\n","            spectral_norm(nn.ConvTranspose2d(nfc*(8+4+2), nfc*16, 4, 1, 0, bias=False)), #4\n","            nn.BatchNorm2d(nfc*16), GLU(), up_gan(nfc*8, nfc*4) )\n","        \n","        self.encode_256 = nn.Sequential( spectral_norm(nn.Conv2d(3, d256, 4, 2, 1, bias=False)),nn.LeakyReLU(0.2,inplace=True))\n","        self.encode_128 = down_gan(d256, d128)\n","        self.encode_64 = down_gan(d128, d64)\n","        self.encode_32 = down_gan(d64, d32)\n","        self.encode_16 = down_gan(d32, d16)\n","\n","        self.residual_16 = nn.Sequential( ResBlkG(d16+nfc*4), Swish(), ResBlkG(d16+nfc*4), Swish() )\n","\n","        self.decode_32  = up_gan(d16+nfc*4, d32)\n","        self.decode_64  = up_gan(d32+nfc, d64) \n","        self.decode_128 = up_gan(d64, d128)\n","        self.decode_256 = up_gan(d128, d256)\n","        self.decode_512 = up_gan(d256, d512)\n","        if im_size == 1024:\n","            self.decode_1024 = up_gan(d512, nfc//4)\n","\n","        self.style_64  =  nn.Sequential( spectral_norm( nn.Linear(nfc*8, d64) ), nn.ReLU(), nn.Linear(d64, d64),  nn.Sigmoid(), UnSqueeze())\n","        self.style_128 =  nn.Sequential( spectral_norm( nn.Linear(nfc*8, d128)), nn.ReLU(), nn.Linear(d128, d128),nn.Sigmoid(), UnSqueeze())\n","        self.style_256 =  nn.Sequential( spectral_norm( nn.Linear(nfc*4, d256)), nn.ReLU(), nn.Linear(d256, d256),nn.Sigmoid(), UnSqueeze())\n","        self.style_512 =  nn.Sequential( spectral_norm( nn.Linear(nfc*2, d512)), nn.ReLU(), nn.Linear(d512, d512),nn.Sigmoid(), UnSqueeze())\n","        \n","        self.to_rgb = nn.Sequential( nn.Conv2d(nfc//2, 3, 3, 1, 1, bias=False), nn.Tanh() )\n","        if im_size == 1024:\n","            self.to_rgb = nn.Sequential( nn.Conv2d(nfc//4, 3, 3, 1, 1, bias=False), nn.Tanh() )\n","        \n","        if DATA_NAME=='shoe':\n","            self.bs_0 = nn.Parameter(torch.randn(1, nfc*2))\n","            self.bs_1 = nn.Parameter(torch.randn(1, nfc*4))\n","            self.bs_2 = nn.Parameter(torch.randn(1, nfc*8))\n","\n","    def forward(self, image, style_vectors):\n","         \n","        s_16 = repeat_upscale( self.from_style(torch.cat(style_vectors,1)), scale_factor=2 )\n","        if DATA_NAME=='shoe':  \n","            s_16 = torch.zeros_like(s_16)\n","            \n","        n_32 = self.from_noise_32(torch.randn_like(style_vectors[2]))\n","\n","        e_256 = self.encode_256( image )\n","        e_128 = self.encode_128( e_256 )\n","        e_64 = self.encode_64( e_128 )\n","        e_32 = self.encode_32( e_64 )\n","        e_16 = self.encode_16(e_32)\n","\n","        e_16 = self.residual_16( torch.cat([e_16, s_16],dim=1) )\n","        \n","        d_32 = self.decode_32( e_16 )\n","        d_64 = self.decode_64( torch.cat([d_32, n_32], dim=1) ) \n","        if DATA_NAME!='shoe':\n","            d_64 *= self.style_64(style_vectors[2])\n","        d_128 = self.decode_128( d_64 + e_64 ) \n","        if DATA_NAME!='shoe':\n","            d_128 *= self.style_128(style_vectors[2])\n","        d_256 = self.decode_256( d_128 + e_128 )\n","        if DATA_NAME!='shoe':\n","            d_256 *= self.style_256(style_vectors[1])\n","        d_512 = self.decode_512( d_256 + e_256 ) \n","        if DATA_NAME!='shoe':\n","            d_512 *= self.style_512(style_vectors[0])\n","        \n","        if self.im_size == 1024:\n","            d_final = self.decode_1024(d_512)\n","        else:\n","            d_final = d_512\n","        return self.to_rgb(d_final)\n","\n","\n","class RefineGenerator_face(nn.Module):\n","    def __init__(self, nfc, im_size):\n","        super().__init__()  \n","\n","        self.im_size = im_size\n","\n","        e1, e2, e3, e4 = 16, 32, 64, 128\n","        self.encode_1 = down_gan(3, e1)      \n","        self.encode_2 = down_gan(e1, e2)     \n","        self.encode_3 = down_gan(e2, e3)     \n","        self.encode_4 = down_gan(e3, e4)    \n","\n","        s1, s2, s3, s4 = 256, 128, 128, 64\n","        self.style = nn.Sequential(nn.Linear(nfc*(8+4+2), 512), nn.LeakyReLU(0.1))\n","        self.from_style_32 = nn.Sequential(\n","            spectral_norm(nn.ConvTranspose2d(512, s1, 4, 1, 0, bias=False)), \n","            nn.BatchNorm2d(s1), GLU(), up_gan(s1//2, s2), up_gan(s2, s3), up_gan(s3, s4)) \n","\n","        d1, d2, d3, d4, d5 = 256, 128, 64, 32, 16\n","        self.decode_64 = up_gan( e4 + s4 , d1)\n","        self.decode_128 = up_gan(d1+e3, d2)\n","        self.decode_256 = up_gan(d2+e2, d3)\n","        self.decode_512 = up_gan(d3+e1, d4)\n","        if im_size == 1024:\n","            self.decode_1024 = up_gan(d4, d5)\n","\n","        self.style_blocks = nn.ModuleList()\n","        \n","        chs = [d1, d2, d3, d4]\n","        if im_size == 1024:\n","            chs.append(d5)\n","        for i in range(len(chs)):\n","            self.style_blocks.append(nn.Sequential( \n","                    nn.Linear(512, chs[i]), nn.ReLU(), nn.Linear(chs[i], chs[i]), nn.Sigmoid() ))\n","\n","        self.final = nn.Sequential( spectral_norm( \n","                            nn.Conv2d(d4, 3, 3, 1, 1, bias=False) ), nn.Tanh() )\n","        if im_size == 1024:\n","            self.final = nn.Sequential( spectral_norm( \n","                            nn.Conv2d(d5, 3, 3, 1, 1, bias=False) ), nn.Tanh() )\n","        \n","    def forward(self, image, style):\n","        e_256 = self.encode_1( image )\n","        e_128 = self.encode_2( e_256 )\n","        e_64 = self.encode_3( e_128 )\n","        e_32 = self.encode_4( e_64 )\n","\n","        style = self.style(torch.cat(style, dim=1))\n","        s_32 = self.from_style_32( style.unsqueeze(-1).unsqueeze(-1) )\n","        \n","        if random.randint(0, 1) == 1:\n","            s_32 = s_32.flip(2)\n","        if random.randint(0, 1) == 1:\n","            s_32 = s_32.flip(3)\n","        \n","        feat_64 = self.decode_64( torch.cat([e_32, s_32], dim=1) ) * self.style_blocks[0](style).unsqueeze(-1).unsqueeze(-1)\n","        feat_128 = self.decode_128( torch.cat([e_64, feat_64], dim=1) ) * self.style_blocks[1](style).unsqueeze(-1).unsqueeze(-1)\n","        feat_256 = self.decode_256( torch.cat([e_128, feat_128], dim=1) ) * self.style_blocks[2](style).unsqueeze(-1).unsqueeze(-1)\n","        feat_512 = self.decode_512( torch.cat([e_256, feat_256], dim=1) ) * self.style_blocks[3](style).unsqueeze(-1).unsqueeze(-1)\n","        if self.im_size == 1024:\n","            feat_1024 = self.decode_1024( feat_512 ) * self.style_blocks[4](style).unsqueeze(-1).unsqueeze(-1)\n","            return self.final(feat_1024)\n","        else:\n","            return self.final(feat_512)\n","\n","\n","class DownBlock(nn.Module):\n","    def __init__(self, ch_in, ch_out, ch_skip=0):\n","        super().__init__()\n","\n","        self.ch_out = ch_out\n","        self.down_main = nn.Sequential(\n","                spectral_norm(nn.Conv2d(ch_in, ch_out, 3, 2, 1, bias=False)),\n","                nn.BatchNorm2d(ch_out),\n","                nn.LeakyReLU(0.2, inplace=True),\n","                spectral_norm(nn.Conv2d(ch_out, ch_out, 3, 1, 1, bias=False)),\n","                nn.BatchNorm2d(ch_out),\n","                nn.LeakyReLU(0.2, inplace=True)\n","                )\n","\n","        self.skip = False\n","        \n","        if ch_skip > 0:  \n","            self.skip = True \n","            self.skip_conv = nn.Sequential(\n","                nn.AdaptiveAvgPool2d(4),\n","                spectral_norm( nn.Conv2d(ch_skip, ch_out, 4, 1, 0, bias=False) ),\n","                nn.ReLU(),\n","                spectral_norm( nn.Conv2d(ch_out, ch_out*2, 1, 1, 0, bias=False) ),\n","            ) \n","\n","    def forward(self, feat, skip_feat=None):\n","        feat_out = self.down_main(feat) \n","        if skip_feat is not None and self.skip:\n","            addon = self.skip_conv(skip_feat)\n","            feat_out = feat_out * torch.sigmoid(addon[:,:self.ch_out]) + torch.tanh(addon[:,self.ch_out:])        \n","        \n","        return feat_out\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, ndf=64, nc=3, im_size=512):\n","        super(Discriminator, self).__init__()\n","        self.ndf = ndf\n","        self.im_size = im_size\n","\n","        modules = [\n","            nn.Sequential(spectral_norm(nn.Conv2d(nc, ndf//4, 4, 2, 1, bias=False)),\n","                          nn.LeakyReLU(0.2, inplace=True)),\n","            DownBlock(ndf//4, ndf//2),\n","            DownBlock(ndf//2, ndf*1),\n","            DownBlock(ndf*1,  ndf*2),\n","            DownBlock(ndf*2,  ndf*4, ch_skip=ndf//4),\n","            ]\n","\n","        if im_size == 512:\n","            modules.append(\n","                DownBlock(ndf*4,  ndf*16, ch_skip=ndf//2),\n","            )\n","        elif im_size == 1024:\n","            modules.append(\n","                DownBlock(ndf*4,  ndf*8, ch_skip=ndf//2))\n","            modules.append(\n","                DownBlock(ndf*8,  ndf*16, ch_skip=ndf*1),\n","            )\n","        modules.append(\n","                        nn.Sequential(\n","                            spectral_norm(nn.Conv2d(ndf*16, ndf*16, 1, 1, 0, bias=False)),\n","                            nn.BatchNorm2d(ndf*16),\n","                            nn.LeakyReLU(0.2, inplace=True),\n","                            spectral_norm(nn.Conv2d(ndf*16, 1, 4, 1, 0, bias=False)))\n","                       )\n","\n","        self.main = nn.ModuleList(modules)\n","        \n","        self.apply(weights_init)\n","\n","\n","    def forward(self, x):\n","        feat_256 = self.main[0](x)\n","        feat_128 = self.main[1](feat_256)\n","        feat_64 = self.main[2](feat_128)\n","        feat_32 = self.main[3](feat_64)\n","\n","        feat_16 = self.main[4](feat_32, feat_256)\n","        feat_8 = self.main[5](feat_16, feat_128)\n","        if self.im_size == 1024:\n","            feat_last = self.main[6](feat_8, feat_64)\n","        else:\n","            feat_last = feat_8\n","\n","        return self.main[-1](feat_last)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"celJMqZQ9E7b","executionInfo":{"status":"ok","timestamp":1634051389503,"user_tz":-330,"elapsed":497,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"cc342ce2-a194-4e66-e380-ddde957709bf"},"source":["import torch\n","import torchvision\n","print(torch.__version__)\n","print(torchvision.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu102\n","0.10.0+cu102\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"Z-2tTBQC963P","executionInfo":{"status":"ok","timestamp":1634051312579,"user_tz":-330,"elapsed":88979,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"d2450d07-d687-4493-833f-c910d348db43"},"source":["!pip3 install torch==1.9.0 torchvision==0.10.0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==1.9.0\n","  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 2.6 kB/s \n","\u001b[?25hCollecting torchvision==0.10.0\n","  Downloading torchvision-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n","\u001b[K     |████████████████████████████████| 22.1 MB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0) (7.1.2)\n","Installing collected packages: torch, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.7.0\n","    Uninstalling torch-1.7.0:\n","      Successfully uninstalled torch-1.7.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.8.1\n","    Uninstalling torchvision-0.8.1:\n","      Successfully uninstalled torchvision-0.8.1\n","Successfully installed torch-1.9.0 torchvision-0.10.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchvision"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"RYNfJbJGcnCI"},"source":["import os\n","import torch\n","from torch.autograd import Variable\n","from pdb import set_trace as st\n","from IPython import embed\n","\n","class BaseModel():\n","    def __init__(self):\n","        pass;\n","        \n","    def name(self):\n","        return 'BaseModel'\n","\n","    def initialize(self, use_gpu=True, gpu_ids=[0]):\n","        self.use_gpu = use_gpu\n","        self.gpu_ids = gpu_ids\n","\n","    def forward(self):\n","        pass\n","\n","    def get_image_paths(self):\n","        pass\n","\n","    def optimize_parameters(self):\n","        pass\n","\n","    def get_current_visuals(self):\n","        return self.input\n","\n","    def get_current_errors(self):\n","        return {}\n","\n","    def save(self, label):\n","        pass\n","\n","    def save_network(self, network, path, network_label, epoch_label):\n","        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","        save_path = os.path.join(path, save_filename)\n","        torch.save(network.state_dict(), save_path)\n","\n","    def load_network(self, network, network_label, epoch_label):\n","        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","        save_path = os.path.join(self.save_dir, save_filename)\n","        print('Loading network from %s'%save_path)\n","        network.load_state_dict(torch.load(save_path))\n","\n","    def update_learning_rate():\n","        pass\n","\n","    def get_image_paths(self):\n","        return self.image_paths\n","\n","    def save_done(self, flag=False):\n","        np.save(os.path.join(self.save_dir, 'done_flag'),flag)\n","        np.savetxt(os.path.join(self.save_dir, 'done_flag'),[flag,],fmt='%i')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6gXwLyqoUh6"},"source":["from collections import namedtuple\n","import torch\n","from torchvision import models as tv\n","from IPython import embed\n","\n","class squeezenet(torch.nn.Module):\n","    def __init__(self, requires_grad=False, pretrained=True):\n","        super(squeezenet, self).__init__()\n","        pretrained_features = tv.squeezenet1_1(pretrained=pretrained).features\n","        self.slice1 = torch.nn.Sequential()\n","        self.slice2 = torch.nn.Sequential()\n","        self.slice3 = torch.nn.Sequential()\n","        self.slice4 = torch.nn.Sequential()\n","        self.slice5 = torch.nn.Sequential()\n","        self.slice6 = torch.nn.Sequential()\n","        self.slice7 = torch.nn.Sequential()\n","        self.N_slices = 7\n","        for x in range(2):\n","            self.slice1.add_module(str(x), pretrained_features[x])\n","        for x in range(2,5):\n","            self.slice2.add_module(str(x), pretrained_features[x])\n","        for x in range(5, 8):\n","            self.slice3.add_module(str(x), pretrained_features[x])\n","        for x in range(8, 10):\n","            self.slice4.add_module(str(x), pretrained_features[x])\n","        for x in range(10, 11):\n","            self.slice5.add_module(str(x), pretrained_features[x])\n","        for x in range(11, 12):\n","            self.slice6.add_module(str(x), pretrained_features[x])\n","        for x in range(12, 13):\n","            self.slice7.add_module(str(x), pretrained_features[x])\n","        if not requires_grad:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, X):\n","        h = self.slice1(X)\n","        h_relu1 = h\n","        h = self.slice2(h)\n","        h_relu2 = h\n","        h = self.slice3(h)\n","        h_relu3 = h\n","        h = self.slice4(h)\n","        h_relu4 = h\n","        h = self.slice5(h)\n","        h_relu5 = h\n","        h = self.slice6(h)\n","        h_relu6 = h\n","        h = self.slice7(h)\n","        h_relu7 = h\n","        vgg_outputs = namedtuple(\"SqueezeOutputs\", ['relu1','relu2','relu3','relu4','relu5','relu6','relu7'])\n","        out = vgg_outputs(h_relu1,h_relu2,h_relu3,h_relu4,h_relu5,h_relu6,h_relu7)\n","\n","        return out\n","\n","\n","class alexnet(torch.nn.Module):\n","    def __init__(self, requires_grad=False, pretrained=True):\n","        super(alexnet, self).__init__()\n","        alexnet_pretrained_features = tv.alexnet(pretrained=pretrained).features\n","        self.slice1 = torch.nn.Sequential()\n","        self.slice2 = torch.nn.Sequential()\n","        self.slice3 = torch.nn.Sequential()\n","        self.slice4 = torch.nn.Sequential()\n","        self.slice5 = torch.nn.Sequential()\n","        self.N_slices = 5\n","        for x in range(2):\n","            self.slice1.add_module(str(x), alexnet_pretrained_features[x])\n","        for x in range(2, 5):\n","            self.slice2.add_module(str(x), alexnet_pretrained_features[x])\n","        for x in range(5, 8):\n","            self.slice3.add_module(str(x), alexnet_pretrained_features[x])\n","        for x in range(8, 10):\n","            self.slice4.add_module(str(x), alexnet_pretrained_features[x])\n","        for x in range(10, 12):\n","            self.slice5.add_module(str(x), alexnet_pretrained_features[x])\n","        if not requires_grad:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, X):\n","        h = self.slice1(X)\n","        h_relu1 = h\n","        h = self.slice2(h)\n","        h_relu2 = h\n","        h = self.slice3(h)\n","        h_relu3 = h\n","        h = self.slice4(h)\n","        h_relu4 = h\n","        h = self.slice5(h)\n","        h_relu5 = h\n","        alexnet_outputs = namedtuple(\"AlexnetOutputs\", ['relu1', 'relu2', 'relu3', 'relu4', 'relu5'])\n","        out = alexnet_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n","\n","        return out\n","\n","class vgg16(torch.nn.Module):\n","    def __init__(self, requires_grad=False, pretrained=True):\n","        super(vgg16, self).__init__()\n","        vgg_pretrained_features = tv.vgg16(pretrained=pretrained).features\n","        self.slice1 = torch.nn.Sequential()\n","        self.slice2 = torch.nn.Sequential()\n","        self.slice3 = torch.nn.Sequential()\n","        self.slice4 = torch.nn.Sequential()\n","        self.slice5 = torch.nn.Sequential()\n","        self.N_slices = 5\n","        for x in range(4):\n","            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(4, 9):\n","            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(9, 16):\n","            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(16, 23):\n","            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n","        for x in range(23, 30):\n","            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n","        if not requires_grad:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, X):\n","        h = self.slice1(X)\n","        h_relu1_2 = h\n","        h = self.slice2(h)\n","        h_relu2_2 = h\n","        h = self.slice3(h)\n","        h_relu3_3 = h\n","        h = self.slice4(h)\n","        h_relu4_3 = h\n","        h = self.slice5(h)\n","        h_relu5_3 = h\n","        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n","        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n","\n","        return out\n","\n","\n","\n","class resnet(torch.nn.Module):\n","    def __init__(self, requires_grad=False, pretrained=True, num=18):\n","        super(resnet, self).__init__()\n","        if(num==18):\n","            self.net = tv.resnet18(pretrained=pretrained)\n","        elif(num==34):\n","            self.net = tv.resnet34(pretrained=pretrained)\n","        elif(num==50):\n","            self.net = tv.resnet50(pretrained=pretrained)\n","        elif(num==101):\n","            self.net = tv.resnet101(pretrained=pretrained)\n","        elif(num==152):\n","            self.net = tv.resnet152(pretrained=pretrained)\n","        self.N_slices = 5\n","\n","        self.conv1 = self.net.conv1\n","        self.bn1 = self.net.bn1\n","        self.relu = self.net.relu\n","        self.maxpool = self.net.maxpool\n","        self.layer1 = self.net.layer1\n","        self.layer2 = self.net.layer2\n","        self.layer3 = self.net.layer3\n","        self.layer4 = self.net.layer4\n","\n","    def forward(self, X):\n","        h = self.conv1(X)\n","        h = self.bn1(h)\n","        h = self.relu(h)\n","        h_relu1 = h\n","        h = self.maxpool(h)\n","        h = self.layer1(h)\n","        h_conv2 = h\n","        h = self.layer2(h)\n","        h_conv3 = h\n","        h = self.layer3(h)\n","        h_conv4 = h\n","        h = self.layer4(h)\n","        h_conv5 = h\n","\n","        outputs = namedtuple(\"Outputs\", ['relu1','conv2','conv3','conv4','conv5'])\n","        out = outputs(h_relu1, h_conv2, h_conv3, h_conv4, h_conv5)\n","\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrbzIk9Upw8W","executionInfo":{"status":"ok","timestamp":1634107345883,"user_tz":-330,"elapsed":3824,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"ff1fb6a6-c210-4ea0-91b5-44c3c2a0e6b1"},"source":["!pip install lpips"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[?25l\r\u001b[K     |██████                          | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 20 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 30 kB 27.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 40 kB 20.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 51 kB 15.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.4.1)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.19.5)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.10.0+cu111)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.62.3)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.9.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->lpips) (3.7.4.3)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (7.1.2)\n","Installing collected packages: lpips\n","Successfully installed lpips-0.1.4\n"]}]},{"cell_type":"code","metadata":{"id":"4E-erI2qorZ5"},"source":["from __future__ import absolute_import\n","\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","from torch.autograd import Variable\n","import numpy as np\n","from pdb import set_trace as st\n","from skimage import color\n","from IPython import embed\n","\n","import lpips as util\n","\n","def spatial_average(in_tens, keepdim=True):\n","    return in_tens.mean([2,3],keepdim=keepdim)\n","\n","def upsample(in_tens, out_H=64):\n","    in_H = in_tens.shape[2]\n","    scale_factor = 1.*out_H/in_H\n","\n","    return nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)(in_tens)\n","\n","class PNetLin(nn.Module):\n","    def __init__(self, pnet_type='vgg', pnet_rand=False, pnet_tune=False, use_dropout=True, spatial=False, version='0.1', lpips=True):\n","        super(PNetLin, self).__init__()\n","\n","        self.pnet_type = pnet_type\n","        self.pnet_tune = pnet_tune\n","        self.pnet_rand = pnet_rand\n","        self.spatial = spatial\n","        self.lpips = lpips\n","        self.version = version\n","        self.scaling_layer = ScalingLayer()\n","\n","        if(self.pnet_type in ['vgg','vgg16']):\n","            net_type = vgg16\n","            self.chns = [64,128,256,512,512]\n","        elif(self.pnet_type=='alex'):\n","            net_type = alexnet\n","            self.chns = [64,192,384,256,256]\n","        elif(self.pnet_type=='squeeze'):\n","            net_type = squeezenet\n","            self.chns = [64,128,256,384,384,512,512]\n","        self.L = len(self.chns)\n","\n","        self.net = net_type(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)\n","\n","        if(lpips):\n","            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n","            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n","            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n","            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n","            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n","            self.lins = [self.lin0,self.lin1,self.lin2,self.lin3,self.lin4]\n","            if(self.pnet_type=='squeeze'):\n","                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)\n","                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)\n","                self.lins+=[self.lin5,self.lin6]\n","\n","    def forward(self, in0, in1, retPerLayer=False):\n","        in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(in1)) if self.version=='0.1' else (in0, in1)\n","        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n","        feats0, feats1, diffs = {}, {}, {}\n","\n","        for kk in range(self.L):\n","            feats0[kk], feats1[kk] = util.normalize_tensor(outs0[kk]), util.normalize_tensor(outs1[kk])\n","            diffs[kk] = (feats0[kk]-feats1[kk])**2\n","\n","        if(self.lpips):\n","            if(self.spatial):\n","                res = [upsample(self.lins[kk].model(diffs[kk]), out_H=in0.shape[2]) for kk in range(self.L)]\n","            else:\n","                res = [spatial_average(self.lins[kk].model(diffs[kk]), keepdim=True) for kk in range(self.L)]\n","        else:\n","            if(self.spatial):\n","                res = [upsample(diffs[kk].sum(dim=1,keepdim=True), out_H=in0.shape[2]) for kk in range(self.L)]\n","            else:\n","                res = [spatial_average(diffs[kk].sum(dim=1,keepdim=True), keepdim=True) for kk in range(self.L)]\n","\n","        val = res[0]\n","        for l in range(1,self.L):\n","            val += res[l]\n","        \n","        if(retPerLayer):\n","            return (val, res)\n","        else:\n","            return val\n","\n","class ScalingLayer(nn.Module):\n","    def __init__(self):\n","        super(ScalingLayer, self).__init__()\n","        self.register_buffer('shift', torch.Tensor([-.030,-.088,-.188])[None,:,None,None])\n","        self.register_buffer('scale', torch.Tensor([.458,.448,.450])[None,:,None,None])\n","\n","    def forward(self, inp):\n","        return (inp - self.shift) / self.scale\n","\n","\n","class NetLinLayer(nn.Module):\n","    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n","        super(NetLinLayer, self).__init__()\n","\n","        layers = [nn.Dropout(),] if(use_dropout) else []\n","        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False),]\n","        self.model = nn.Sequential(*layers)\n","\n","\n","class Dist2LogitLayer(nn.Module):\n","    def __init__(self, chn_mid=32, use_sigmoid=True):\n","        super(Dist2LogitLayer, self).__init__()\n","\n","        layers = [nn.Conv2d(5, chn_mid, 1, stride=1, padding=0, bias=True),]\n","        layers += [nn.LeakyReLU(0.2,True),]\n","        layers += [nn.Conv2d(chn_mid, chn_mid, 1, stride=1, padding=0, bias=True),]\n","        layers += [nn.LeakyReLU(0.2,True),]\n","        layers += [nn.Conv2d(chn_mid, 1, 1, stride=1, padding=0, bias=True),]\n","        if(use_sigmoid):\n","            layers += [nn.Sigmoid(),]\n","        self.model = nn.Sequential(*layers)\n","\n","    def forward(self,d0,d1,eps=0.1):\n","        return self.model.forward(torch.cat((d0,d1,d0-d1,d0/(d1+eps),d1/(d0+eps)),dim=1))\n","\n","class BCERankingLoss(nn.Module):\n","    def __init__(self, chn_mid=32):\n","        super(BCERankingLoss, self).__init__()\n","        self.net = Dist2LogitLayer(chn_mid=chn_mid)\n","        self.loss = torch.nn.BCELoss()\n","\n","    def forward(self, d0, d1, judge):\n","        per = (judge+1.)/2.\n","        self.logit = self.net.forward(d0,d1)\n","        return self.loss(self.logit, per)\n","\n","class FakeNet(nn.Module):\n","    def __init__(self, use_gpu=True, colorspace='Lab'):\n","        super(FakeNet, self).__init__()\n","        self.use_gpu = use_gpu\n","        self.colorspace=colorspace\n","\n","class L2(FakeNet):\n","\n","    def forward(self, in0, in1, retPerLayer=None):\n","        assert(in0.size()[0]==1)\n","\n","        if(self.colorspace=='RGB'):\n","            (N,C,X,Y) = in0.size()\n","            value = torch.mean(torch.mean(torch.mean((in0-in1)**2,dim=1).view(N,1,X,Y),dim=2).view(N,1,1,Y),dim=3).view(N)\n","            return value\n","        elif(self.colorspace=='Lab'):\n","            value = util.l2(util.tensor2np(util.tensor2tensorlab(in0.data,to_norm=False)), \n","                util.tensor2np(util.tensor2tensorlab(in1.data,to_norm=False)), range=100.).astype('float')\n","            ret_var = Variable( torch.Tensor((value,) ) )\n","            if(self.use_gpu):\n","                ret_var = ret_var.cuda()\n","            return ret_var\n","\n","class DSSIM(FakeNet):\n","\n","    def forward(self, in0, in1, retPerLayer=None):\n","        assert(in0.size()[0]==1)\n","\n","        if(self.colorspace=='RGB'):\n","            value = util.dssim(1.*util.tensor2im(in0.data), 1.*util.tensor2im(in1.data), range=255.).astype('float')\n","        elif(self.colorspace=='Lab'):\n","            value = util.dssim(util.tensor2np(util.tensor2tensorlab(in0.data,to_norm=False)), \n","                util.tensor2np(util.tensor2tensorlab(in1.data,to_norm=False)), range=100.).astype('float')\n","        ret_var = Variable( torch.Tensor((value,) ) )\n","        if(self.use_gpu):\n","            ret_var = ret_var.cuda()\n","        return ret_var\n","\n","def print_network(net):\n","    num_params = 0\n","    for param in net.parameters():\n","        num_params += param.numel()\n","    print('Network',net)\n","    print('Total number of parameters: %d' % num_params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7KPHacNp5W5"},"source":["from __future__ import absolute_import\n","import sys\n","import numpy as np\n","import torch\n","from torch import nn\n","import os\n","from collections import OrderedDict\n","from torch.autograd import Variable\n","import itertools\n","from scipy.ndimage import zoom\n","import fractions\n","import functools\n","import skimage.transform\n","from tqdm import tqdm\n","\n","from IPython import embed\n","import lpips as util\n","\n","class DistModel(BaseModel):\n","    def name(self):\n","        return self.model_name\n","\n","    def initialize(self, model='net-lin', net='alex', colorspace='Lab', pnet_rand=False, pnet_tune=False, model_path=None,\n","            use_gpu=True, printNet=False, spatial=False, \n","            is_train=False, lr=.0001, beta1=0.5, version='0.1', gpu_ids=[0]):\n","        \n","        BaseModel.initialize(self, use_gpu=use_gpu, gpu_ids=gpu_ids)\n","\n","        self.model = model\n","        self.net = net\n","        self.is_train = is_train\n","        self.spatial = spatial\n","        self.gpu_ids = gpu_ids\n","        self.model_name = '%s [%s]'%(model,net)\n","\n","        if(self.model == 'net-lin'):\n","            self.net = PNetLin(pnet_rand=pnet_rand, pnet_tune=pnet_tune, pnet_type=net,\n","                use_dropout=True, spatial=spatial, version=version, lpips=True)\n","            kw = {}\n","            if not use_gpu:\n","                kw['map_location'] = 'cpu'\n","            if(model_path is None):\n","                import inspect\n","                model_path = os.path.abspath(os.path.join(inspect.getfile(self.initialize), '..', 'weights/v%s/%s.pth'%(version,net)))\n","\n","            if(not is_train):\n","                print('Loading model from: %s'%model_path)\n","                self.net.load_state_dict(torch.load(model_path, **kw), strict=False)\n","\n","        elif(self.model=='net'):\n","            self.net = PNetLin(pnet_rand=pnet_rand, pnet_type=net, lpips=False)\n","        elif(self.model in ['L2','l2']):\n","            self.net = L2(use_gpu=use_gpu,colorspace=colorspace)\n","            self.model_name = 'L2'\n","        elif(self.model in ['DSSIM','dssim','SSIM','ssim']):\n","            self.net = DSSIM(use_gpu=use_gpu,colorspace=colorspace)\n","            self.model_name = 'SSIM'\n","        else:\n","            raise ValueError(\"Model [%s] not recognized.\" % self.model)\n","\n","        self.parameters = list(self.net.parameters())\n","\n","        if self.is_train:\n","            self.rankLoss = BCERankingLoss()\n","            self.parameters += list(self.rankLoss.net.parameters())\n","            self.lr = lr\n","            self.old_lr = lr\n","            self.optimizer_net = torch.optim.Adam(self.parameters, lr=lr, betas=(beta1, 0.999))\n","        else:\n","            self.net.eval()\n","\n","        if(use_gpu):\n","            self.net.to(gpu_ids[0])\n","            self.net = torch.nn.DataParallel(self.net, device_ids=gpu_ids)\n","            if(self.is_train):\n","                self.rankLoss = self.rankLoss.to(device=gpu_ids[0])\n","\n","        if(printNet):\n","            print('---------- Networks initialized -------------')\n","            print_network(self.net)\n","            print('-----------------------------------------------')\n","\n","    def forward(self, in0, in1, retPerLayer=False):\n","\n","        return self.net.forward(in0, in1, retPerLayer=retPerLayer)\n","\n","    def optimize_parameters(self):\n","        self.forward_train()\n","        self.optimizer_net.zero_grad()\n","        self.backward_train()\n","        self.optimizer_net.step()\n","        self.clamp_weights()\n","\n","    def clamp_weights(self):\n","        for module in self.net.modules():\n","            if(hasattr(module, 'weight') and module.kernel_size==(1,1)):\n","                module.weight.data = torch.clamp(module.weight.data,min=0)\n","\n","    def set_input(self, data):\n","        self.input_ref = data['ref']\n","        self.input_p0 = data['p0']\n","        self.input_p1 = data['p1']\n","        self.input_judge = data['judge']\n","\n","        if(self.use_gpu):\n","            self.input_ref = self.input_ref.to(device=self.gpu_ids[0])\n","            self.input_p0 = self.input_p0.to(device=self.gpu_ids[0])\n","            self.input_p1 = self.input_p1.to(device=self.gpu_ids[0])\n","            self.input_judge = self.input_judge.to(device=self.gpu_ids[0])\n","\n","        self.var_ref = Variable(self.input_ref,requires_grad=True)\n","        self.var_p0 = Variable(self.input_p0,requires_grad=True)\n","        self.var_p1 = Variable(self.input_p1,requires_grad=True)\n","\n","    def forward_train(self):\n","        self.d0 = self.forward(self.var_ref, self.var_p0)\n","        self.d1 = self.forward(self.var_ref, self.var_p1)\n","        self.acc_r = self.compute_accuracy(self.d0,self.d1,self.input_judge)\n","\n","        self.var_judge = Variable(1.*self.input_judge).view(self.d0.size())\n","\n","        self.loss_total = self.rankLoss.forward(self.d0, self.d1, self.var_judge*2.-1.)\n","\n","        return self.loss_total\n","\n","    def backward_train(self):\n","        torch.mean(self.loss_total).backward()\n","\n","    def compute_accuracy(self,d0,d1,judge):\n","        d1_lt_d0 = (d1<d0).cpu().data.numpy().flatten()\n","        judge_per = judge.cpu().numpy().flatten()\n","        return d1_lt_d0*judge_per + (1-d1_lt_d0)*(1-judge_per)\n","\n","    def get_current_errors(self):\n","        retDict = OrderedDict([('loss_total', self.loss_total.data.cpu().numpy()),\n","                            ('acc_r', self.acc_r)])\n","\n","        for key in retDict.keys():\n","            retDict[key] = np.mean(retDict[key])\n","\n","        return retDict\n","\n","    def get_current_visuals(self):\n","        zoom_factor = 256/self.var_ref.data.size()[2]\n","\n","        ref_img = util.tensor2im(self.var_ref.data)\n","        p0_img = util.tensor2im(self.var_p0.data)\n","        p1_img = util.tensor2im(self.var_p1.data)\n","\n","        ref_img_vis = zoom(ref_img,[zoom_factor, zoom_factor, 1],order=0)\n","        p0_img_vis = zoom(p0_img,[zoom_factor, zoom_factor, 1],order=0)\n","        p1_img_vis = zoom(p1_img,[zoom_factor, zoom_factor, 1],order=0)\n","\n","        return OrderedDict([('ref', ref_img_vis),\n","                            ('p0', p0_img_vis),\n","                            ('p1', p1_img_vis)])\n","\n","    def save(self, path, label):\n","        if(self.use_gpu):\n","            self.save_network(self.net.module, path, '', label)\n","        else:\n","            self.save_network(self.net, path, '', label)\n","        self.save_network(self.rankLoss.net, path, 'rank', label)\n","\n","    def update_learning_rate(self,nepoch_decay):\n","        lrd = self.lr / nepoch_decay\n","        lr = self.old_lr - lrd\n","\n","        for param_group in self.optimizer_net.param_groups:\n","            param_group['lr'] = lr\n","\n","        print('update lr [%s] decay: %f -> %f' % (type,self.old_lr, lr))\n","        self.old_lr = lr\n","\n","def score_2afc_dataset(data_loader, func, name=''):\n","\n","    d0s = []\n","    d1s = []\n","    gts = []\n","\n","    for data in tqdm(data_loader.load_data(), desc=name):\n","        d0s+=func(data['ref'],data['p0']).data.cpu().numpy().flatten().tolist()\n","        d1s+=func(data['ref'],data['p1']).data.cpu().numpy().flatten().tolist()\n","        gts+=data['judge'].cpu().numpy().flatten().tolist()\n","\n","    d0s = np.array(d0s)\n","    d1s = np.array(d1s)\n","    gts = np.array(gts)\n","    scores = (d0s<d1s)*(1.-gts) + (d1s<d0s)*gts + (d1s==d0s)*.5\n","\n","    return(np.mean(scores), dict(d0s=d0s,d1s=d1s,gts=gts,scores=scores))\n","\n","def score_jnd_dataset(data_loader, func, name=''):\n","\n","    ds = []\n","    gts = []\n","\n","    for data in tqdm(data_loader.load_data(), desc=name):\n","        ds+=func(data['p0'],data['p1']).data.cpu().numpy().tolist()\n","        gts+=data['same'].cpu().numpy().flatten().tolist()\n","\n","    sames = np.array(gts)\n","    ds = np.array(ds)\n","\n","    sorted_inds = np.argsort(ds)\n","    ds_sorted = ds[sorted_inds]\n","    sames_sorted = sames[sorted_inds]\n","\n","    TPs = np.cumsum(sames_sorted)\n","    FPs = np.cumsum(1-sames_sorted)\n","    FNs = np.sum(sames_sorted)-TPs\n","\n","    precs = TPs/(TPs+FPs)\n","    recs = TPs/(TPs+FNs)\n","    score = util.voc_ap(recs,precs)\n","\n","    return(score, dict(ds=ds,sames=sames))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rRjyF2ezrPW5"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import numpy as np\n","from skimage.measure import compare_ssim\n","import torch\n","from torch.autograd import Variable\n","\n","class PerceptualLoss(torch.nn.Module):\n","    def __init__(self, model='net-lin', net='alex', colorspace='rgb', spatial=False, use_gpu=True, gpu_ids=[0]):\n","        super(PerceptualLoss, self).__init__()\n","        print('Setting up Perceptual loss...')\n","        self.use_gpu = use_gpu\n","        self.spatial = spatial\n","        self.gpu_ids = gpu_ids\n","        self.model = DistModel()\n","        self.model.initialize(model=model, net=net, use_gpu=use_gpu, colorspace=colorspace, spatial=self.spatial, gpu_ids=gpu_ids)\n","        print('...[%s] initialized'%self.model.name())\n","        print('...Done')\n","\n","    def forward(self, pred, target, normalize=False):\n","\n","        if normalize:\n","            target = 2 * target  - 1\n","            pred = 2 * pred  - 1\n","\n","        return self.model.forward(target, pred)\n","\n","def normalize_tensor(in_feat,eps=1e-10):\n","    norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1,keepdim=True))\n","    return in_feat/(norm_factor+eps)\n","\n","def l2(p0, p1, range=255.):\n","    return .5*np.mean((p0 / range - p1 / range)**2)\n","\n","def psnr(p0, p1, peak=255.):\n","    return 10*np.log10(peak**2/np.mean((1.*p0-1.*p1)**2))\n","\n","def dssim(p0, p1, range=255.):\n","    return (1 - compare_ssim(p0, p1, data_range=range, multichannel=True)) / 2.\n","\n","def rgb2lab(in_img,mean_cent=False):\n","    from skimage import color\n","    img_lab = color.rgb2lab(in_img)\n","    if(mean_cent):\n","        img_lab[:,:,0] = img_lab[:,:,0]-50\n","    return img_lab\n","\n","def tensor2np(tensor_obj):\n","    return tensor_obj[0].cpu().float().numpy().transpose((1,2,0))\n","\n","def np2tensor(np_obj):\n","    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n","\n","def tensor2tensorlab(image_tensor,to_norm=True,mc_only=False):\n","    from skimage import color\n","\n","    img = tensor2im(image_tensor)\n","    img_lab = color.rgb2lab(img)\n","    if(mc_only):\n","        img_lab[:,:,0] = img_lab[:,:,0]-50\n","    if(to_norm and not mc_only):\n","        img_lab[:,:,0] = img_lab[:,:,0]-50\n","        img_lab = img_lab/100.\n","\n","    return np2tensor(img_lab)\n","\n","def tensorlab2tensor(lab_tensor,return_inbnd=False):\n","    from skimage import color\n","    import warnings\n","    warnings.filterwarnings(\"ignore\")\n","\n","    lab = tensor2np(lab_tensor)*100.\n","    lab[:,:,0] = lab[:,:,0]+50\n","\n","    rgb_back = 255.*np.clip(color.lab2rgb(lab.astype('float')),0,1)\n","    if(return_inbnd):\n","        lab_back = color.rgb2lab(rgb_back.astype('uint8'))\n","        mask = 1.*np.isclose(lab_back,lab,atol=2.)\n","        mask = np2tensor(np.prod(mask,axis=2)[:,:,np.newaxis])\n","        return (im2tensor(rgb_back),mask)\n","    else:\n","        return im2tensor(rgb_back)\n","\n","def rgb2lab(input):\n","    from skimage import color\n","    return color.rgb2lab(input / 255.)\n","\n","def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n","    image_numpy = image_tensor[0].cpu().float().numpy()\n","    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n","    return image_numpy.astype(imtype)\n","\n","def im2tensor(image, imtype=np.uint8, cent=1., factor=255./2.):\n","    return torch.Tensor((image / factor - cent)\n","                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n","\n","def tensor2vec(vector_tensor):\n","    return vector_tensor.data.cpu().numpy()[:, :, 0, 0]\n","\n","def voc_ap(rec, prec, use_07_metric=False):\n","    if use_07_metric:\n","        ap = 0.\n","        for t in np.arange(0., 1.1, 0.1):\n","            if np.sum(rec >= t) == 0:\n","                p = 0\n","            else:\n","                p = np.max(prec[rec >= t])\n","            ap = ap + p / 11.\n","    else:\n","        mrec = np.concatenate(([0.], rec, [1.]))\n","        mpre = np.concatenate(([0.], prec, [0.]))\n","\n","        for i in range(mpre.size - 1, 0, -1):\n","            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n","\n","        i = np.where(mrec[1:] != mrec[:-1])[0]\n","        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n","    return ap\n","\n","def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n","    image_numpy = image_tensor[0].cpu().float().numpy()\n","    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n","    return image_numpy.astype(imtype)\n","\n","def im2tensor(image, imtype=np.uint8, cent=1., factor=255./2.):\n","    return torch.Tensor((image / factor - cent)\n","                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JU9dMmawSJ_"},"source":["import torch\n","import torch.nn.functional as F\n","from torch import optim\n","from torch.utils.data import DataLoader\n","from torchvision import utils as vutils\n","\n","import random\n","from tqdm import tqdm\n","\n","def loss_for_list(loss, fl1, fl2, detach_second=True):\n","    result_loss = 0\n","    for f_idx in range(len(fl1)):\n","        if detach_second:\n","            result_loss += loss( fl1[f_idx] , fl2[f_idx].detach() )\n","        else:\n","            result_loss += loss( fl1[f_idx] , fl2[f_idx] )\n","    return result_loss\n","\n","\n","def loss_for_list_perm(loss, fl1, fl2, detach_second=True):\n","    result_loss = 0\n","    for f_idx in range(len(fl1)):\n","        perm = true_randperm(fl1[0].shape[0], fl1[0].device)\n","        if detach_second:\n","            result_loss += F.relu( 2 + loss( fl1[f_idx] , fl2[f_idx].detach() ) - loss( fl1[f_idx][perm] , fl2[f_idx].detach() ))\n","        else:\n","            result_loss += F.relu( 2 + loss( fl1[f_idx] , fl2[f_idx] ) - loss( fl1[f_idx][perm] , fl2[f_idx] ))\n","    return result_loss\n","\n","\n","def loss_for_list_mean(feat_list):\n","    loss = 0\n","    for feat in feat_list:\n","        if len(feat.shape) == 4:\n","            feat = feat.mean(dim=[2,3])\n","            loss += F.l1_loss( feat, torch.ones_like(feat) )\n","        else:\n","            loss += F.l1_loss( feat, torch.zeros_like(feat) )\n","    return loss\n","\n","\n","def train_ae():\n","\n","    dataset = PairedMultiDataset(data_root_colorful, data_root_sketch_1, data_root_sketch_2, data_root_sketch_3, im_size=IM_SIZE_AE, rand_crop=True)\n","    print(len(dataset))\n","    dataloader = iter(DataLoader(dataset, BATCH_SIZE_AE, \\\n","        sampler=InfiniteSamplerWrapper(dataset), num_workers=DATALOADER_WORKERS, pin_memory=True))\n","\n","\n","    dataset_ss = SelfSupervisedDataset(data_root_colorful, data_root_sketch_3, im_size=IM_SIZE_AE, nbr_cls=NBR_CLS, rand_crop=True)\n","    print(len(dataset_ss), len(dataset_ss.frame))\n","    dataloader_ss = iter(DataLoader(dataset_ss, BATCH_SIZE_AE, \\\n","        sampler=InfiniteSamplerWrapper(dataset_ss), num_workers=DATALOADER_WORKERS, pin_memory=True))\n","\n","\n","    style_encoder = StyleEncoder(nfc=NFC, nbr_cls=NBR_CLS).cuda()\n","    content_encoder = ContentEncoder(nfc=NFC).cuda()\n","    decoder = Decoder(nfc=NFC).cuda()\n","\n","    opt_c = optim.Adam(content_encoder.parameters(), lr=2e-4, betas=(0.5, 0.999))\n","    opt_s = optim.Adam( style_encoder.parameters(), lr=2e-4, betas=(0.5, 0.999))\n","    opt_d = optim.Adam(decoder.parameters(), lr=2e-4, betas=(0.5, 0.999))\n","\n","    style_encoder.reset_cls()\n","    style_encoder.final_cls.cuda()\n","\n","    PRETRAINED_AE_PATH = None\n","    PRETRAINED_AE_ITER = 12000\n","\n","    if PRETRAINED_AE_PATH is not None:\n","        PRETRAINED_AE_PATH = PRETRAINED_AE_PATH + '/models/%d.pth'%PRETRAINED_AE_ITER \n","        ckpt = torch.load(PRETRAINED_AE_PATH)\n","        \n","        print(PRETRAINED_AE_PATH)\n","        \n","        style_encoder.load_state_dict(ckpt['s'])\n","        content_encoder.load_state_dict(ckpt['c'])\n","        decoder.load_state_dict(ckpt['d'])\n","\n","        opt_c.load_state_dict(ckpt['opt_c'])\n","        opt_s.load_state_dict(ckpt['opt_s'])\n","        opt_d.load_state_dict(ckpt['opt_d'])\n","        print('loaded pre-trained AE')\n","    \n","    style_encoder.reset_cls()\n","    style_encoder.final_cls.cuda()\n","    opt_s_cls = optim.Adam( style_encoder.final_cls.parameters(), lr=2e-4, betas=(0.5, 0.999))\n","\n","\n","    saved_image_folder, saved_model_folder = make_folders(SAVE_FOLDER, 'AE_'+TRIAL_NAME)\n","    log_file_path = saved_image_folder+'/../ae_log.txt'\n","    log_file = open(log_file_path, 'w')\n","    log_file.close()\n","    losses_sf_consist = AverageMeter()\n","    losses_cf_consist = AverageMeter()\n","    losses_cls = AverageMeter()\n","    losses_rec_rd = AverageMeter()\n","    losses_rec_org = AverageMeter()\n","    losses_rec_grey = AverageMeter()\n","\n","    percept = PerceptualLoss(model='net-lin', net='vgg', use_gpu=True)\n","\n","    for iteration in tqdm(range(ITERATION_AE)):\n","        \n","        if iteration%( (NBR_CLS*100)//BATCH_SIZE_AE )==0 and iteration>1:\n","            dataset_ss._next_set()\n","            dataloader_ss = iter(DataLoader(dataset_ss, BATCH_SIZE_AE, sampler=InfiniteSamplerWrapper(dataset_ss), num_workers=DATALOADER_WORKERS, pin_memory=True))\n","            style_encoder.reset_cls()\n","            opt_s_cls = optim.Adam( style_encoder.final_cls.parameters(), lr=2e-4, betas=(0.5, 0.999))\n","            \n","            opt_s.param_groups[0]['lr'] = 1e-4\n","            opt_d.param_groups[0]['lr'] = 1e-4\n","\n","        \n","        rgb_img_rd, rgb_img_org, skt_org, skt_bold, skt_erased, skt_erased_bold, img_idx = next(dataloader_ss)\n","        rgb_img_rd = rgb_img_rd.cuda()\n","        rgb_img_org = rgb_img_org.cuda()\n","        img_idx = img_idx.cuda()\n","\n","        skt_org = F.interpolate( skt_org , size=512 ).cuda()\n","        skt_bold = F.interpolate( skt_bold , size=512 ).cuda()\n","        skt_erased = F.interpolate( skt_erased , size=512 ).cuda()\n","        skt_erased_bold = F.interpolate( skt_erased_bold , size=512 ).cuda()\n","\n","        style_encoder.zero_grad()\n","        decoder.zero_grad()\n","        content_encoder.zero_grad()\n","\n","        style_vector_rd, pred_cls_rd = style_encoder(rgb_img_rd)\n","        style_vector_org, pred_cls_org = style_encoder(rgb_img_org)\n","        \n","        content_feats = content_encoder(skt_org)\n","        content_feats_bold = content_encoder(skt_bold)\n","        content_feats_erased = content_encoder(skt_erased)\n","        content_feats_eb = content_encoder(skt_erased_bold)\n","        \n","        rd = random.randint(0, 3)\n","        gimg_rd = None\n","        if rd==0:\n","            gimg_rd = decoder(content_feats, style_vector_rd)\n","        elif rd==1:\n","            gimg_rd = decoder(content_feats_bold, style_vector_rd)\n","        elif rd==2:\n","            gimg_rd = decoder(content_feats_erased, style_vector_rd)\n","        elif rd==3:\n","            gimg_rd = decoder(content_feats_eb, style_vector_rd)\n","\n","\n","        loss_cf_consist = loss_for_list_perm(F.mse_loss, content_feats_bold, content_feats) +\\\n","                            loss_for_list_perm(F.mse_loss, content_feats_erased, content_feats) +\\\n","                                loss_for_list_perm(F.mse_loss, content_feats_eb, content_feats)\n","\n","        loss_sf_consist = 0\n","        for loss_idx in range(3):\n","            loss_sf_consist += -F.cosine_similarity(style_vector_rd[loss_idx], style_vector_org[loss_idx].detach()).mean() + \\\n","                                    F.cosine_similarity(style_vector_rd[loss_idx], style_vector_org[loss_idx][torch.randperm(BATCH_SIZE_AE)].detach()).mean()\n","        \n","        loss_cls = F.cross_entropy(pred_cls_rd, img_idx) + F.cross_entropy(pred_cls_org, img_idx)\n","        loss_rec_rd = F.mse_loss(gimg_rd, rgb_img_org)\n","        if DATA_NAME != 'shoe':\n","            loss_rec_rd += percept( F.adaptive_avg_pool2d(gimg_rd, output_size=256), F.adaptive_avg_pool2d(rgb_img_org, output_size=256)).sum()                \n","        else:\n","            loss_rec_rd += F.l1_loss(gimg_rd, rgb_img_org)\n","        \n","        loss_total = loss_cls + loss_sf_consist + loss_rec_rd + loss_cf_consist \n","        loss_total.backward()\n","\n","        opt_s.step()\n","        opt_s_cls.step()\n","        opt_c.step()\n","        opt_d.step()\n","        \n","        rgb_img, skt_img_1, skt_img_2, skt_img_3 = next(dataloader)\n","            \n","        rgb_img = rgb_img.cuda()\n","\n","        rd = random.randint(0, 3) \n","        if rd == 0:\n","            skt_img = skt_img_1\n","        elif rd == 1:\n","            skt_img = skt_img_2\n","        else:\n","            skt_img = skt_img_3\n","\n","        skt_img = F.interpolate(skt_img, size=512).cuda()\n","\n","        style_encoder.zero_grad()\n","        decoder.zero_grad()\n","        content_encoder.zero_grad()\n","\n","        style_vector, _ = style_encoder(rgb_img)\n","        content_feats = content_encoder(skt_img)\n","        gimg = decoder(content_feats, style_vector)\n","\n","        loss_rec_org = F.mse_loss(gimg, rgb_img)\n","        if DATA_NAME != 'shoe':\n","            loss_rec_org += percept( F.adaptive_avg_pool2d(gimg, output_size=256), \n","                                F.adaptive_avg_pool2d(rgb_img, output_size=256)).sum()\n","            \n","        loss_rec = loss_rec_org \n","        if DATA_NAME == 'shoe':\n","            perm = true_randperm(BATCH_SIZE_AE)\n","            gimg_perm = decoder(content_feats, [s[perm] for s in style_vector])\n","            gimg_grey = gimg_perm.mean(dim=1, keepdim=True)\n","            real_grey = rgb_img.mean(dim=1, keepdim=True)\n","            loss_rec_grey = F.mse_loss( gimg_grey , real_grey )\n","            loss_rec += loss_rec_grey \n","        loss_rec.backward()\n","\n","        opt_s.step()\n","        opt_d.step()\n","        opt_c.step()\n","\n","        losses_cf_consist.update(loss_cf_consist.mean().item(), BATCH_SIZE_AE)\n","        losses_sf_consist.update(loss_sf_consist.mean().item(), BATCH_SIZE_AE)\n","        losses_cls.update(loss_cls.mean().item(), BATCH_SIZE_AE)\n","        losses_rec_rd.update(loss_rec_rd.item(), BATCH_SIZE_AE)\n","        losses_rec_org.update(loss_rec_org.item(), BATCH_SIZE_AE)\n","        if DATA_NAME=='shoe':\n","            losses_rec_grey.update(loss_rec_grey.item(), BATCH_SIZE_AE)\n","\n","\n","        if iteration%LOG_INTERVAL==0:\n","            log_msg = 'Train Stage 1: AE: \\nrec_rd: %.4f  rec_org: %.4f  cls: %.4f  style_consist: %.4f  content_consist: %.4f  rec_grey: %.4f'%(losses_rec_rd.avg, \\\n","                    losses_rec_org.avg, losses_cls.avg, losses_sf_consist.avg, losses_cf_consist.avg, losses_rec_grey.avg)\n","            \n","            print(log_msg)\n","\n","            if log_file_path is not None:\n","                log_file = open(log_file_path, 'a')\n","                log_file.write(log_msg+'\\n')\n","                log_file.close()\n","\n","            losses_sf_consist.reset()\n","            losses_cls.reset()\n","            losses_rec_rd.reset()\n","            losses_rec_org.reset()\n","            losses_cf_consist.reset()\n","            losses_rec_grey.reset()\n","\n","        if iteration%SAVE_IMAGE_INTERVAL==0:\n","            vutils.save_image( torch.cat([rgb_img_rd, F.interpolate(skt_org.repeat(1,3,1,1), size=512) , gimg_rd]), '%s/rd_%d.jpg'%(saved_image_folder, iteration), normalize=True, range=(-1,1) )\n","            if DATA_NAME != 'shoe':\n","                with torch.no_grad():\n","                    perm = true_randperm(BATCH_SIZE_AE)\n","                    gimg_perm = decoder([c for c in content_feats], [s[perm] for s in style_vector])\n","            vutils.save_image( torch.cat([rgb_img, F.interpolate(skt_img.repeat(1,3,1,1), size=512), gimg, gimg_perm]), '%s/org_%d.jpg'%(saved_image_folder, iteration), normalize=True, range=(-1,1) )\n","\n","        if iteration%SAVE_MODEL_INTERVAL==0:\n","            print('Saving history model')\n","            torch.save( {'s': style_encoder.state_dict(),\n","                        'd': decoder.state_dict(),\n","                        'c': content_encoder.state_dict(),\n","                        'opt_c': opt_c.state_dict(),\n","                        'opt_s_cls': opt_s_cls.state_dict(),\n","                        'opt_s': opt_s.state_dict(), \n","                        'opt_d': opt_d.state_dict(),\n","                            }, '%s/%d.pth'%(saved_model_folder, iteration))\n","    \n","    torch.save( {'s': style_encoder.state_dict(),\n","                        'd': decoder.state_dict(),\n","                        'c': content_encoder.state_dict(),\n","                        'opt_c': opt_c.state_dict(),\n","                        'opt_s_cls': opt_s_cls.state_dict(),\n","                        'opt_s': opt_s.state_dict(), \n","                        'opt_d': opt_d.state_dict(),\n","                            }, '%s/%d.pth'%(saved_model_folder, ITERATION_AE))\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jl_-PUe1xgsO","executionInfo":{"status":"ok","timestamp":1634107387461,"user_tz":-330,"elapsed":551,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"046bf5e0-8ae6-46f3-9762-923755bc9a41"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["My_drive  sample_data\n"]}]},{"cell_type":"code","metadata":{"id":"-k8PNX9ixkpg"},"source":["!mkdir weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u9flru3Px07Z"},"source":["!mkdir weights/v0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"caaYS8KiygTo"},"source":["!mkdir weights/v0.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDrTGB-lzzra","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1860d5dbbeb846d98426f4b9883c2329","7c4d892073144037bdd92c486323820f","0be574fcbc4e42549ec46857ba4cc64b","4e8143c8bda048f98a7f40f933f426f3","81d73d544d4c4dba90290f186c1e835a","a779fb20c29d4b42a353417f2791093a","39fd018d292342a3872f1b8dcf605346","bc06500725674dc6a24f862353b9e8be","0e710cd41a4b40508efd43f470fa821e","0c91d8b3206c4207a88c1956589768a6","4e3c88e85d8e45859f762c9e39125b4c"]},"executionInfo":{"status":"ok","timestamp":1634108229414,"user_tz":-330,"elapsed":784746,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"25f9491f-e371-464d-d452-93bec74fc15d"},"source":["train_ae()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["621\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["25 621\n","Setting up Perceptual loss...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1860d5dbbeb846d98426f4b9883c2329","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/528M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loading model from: /content/weights/v0.1/vgg.pth\n","...[net-lin [vgg]] initialized\n","...Done\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/250 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n","  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 6.8075  rec_org: 6.9349  cls: 6.4574  style_consist: -0.2744  content_consist: 17.7025  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n","  warnings.warn(warning)\n"]},{"output_type":"stream","name":"stdout","text":["Saving history model\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 10/250 [00:28<11:58,  2.99s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 6.2104  rec_org: 6.1814  cls: 6.3548  style_consist: -1.3483  content_consist: 12.5245  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 20/250 [01:07<12:46,  3.33s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 5.5885  rec_org: 5.5994  cls: 5.5251  style_consist: -1.7936  content_consist: 0.5564  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 10%|█         | 25/250 [01:31<16:27,  4.39s/it]"]},{"output_type":"stream","name":"stdout","text":["Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 30/250 [01:50<14:53,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 5.2456  rec_org: 5.3962  cls: 3.9595  style_consist: -2.0250  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 40/250 [02:26<11:37,  3.32s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 5.0454  rec_org: 5.1442  cls: 2.4294  style_consist: -2.0636  content_consist: 0.0277  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 50/250 [03:09<13:50,  4.15s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.9796  rec_org: 5.2178  cls: 1.4169  style_consist: -2.2404  content_consist: 0.0000  rec_grey: 0.0000\n","Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 60/250 [03:46<10:22,  3.28s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.8234  rec_org: 4.9810  cls: 1.0604  style_consist: -2.3526  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 70/250 [04:29<13:06,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.7502  rec_org: 4.8336  cls: 0.8187  style_consist: -2.2954  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 75/250 [04:48<11:13,  3.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 80/250 [05:04<08:52,  3.13s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.6044  rec_org: 4.8657  cls: 1.2272  style_consist: -2.4151  content_consist: 0.0054  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 90/250 [05:30<06:38,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.5464  rec_org: 4.6606  cls: 0.8970  style_consist: -2.4180  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 100/250 [05:55<06:12,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.4639  rec_org: 4.6732  cls: 0.3976  style_consist: -2.4249  content_consist: 0.0000  rec_grey: 0.0000\n","Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 110/250 [06:22<05:49,  2.50s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.3206  rec_org: 4.5696  cls: 0.7144  style_consist: -2.2830  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 120/250 [06:47<05:21,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.3936  rec_org: 4.5237  cls: 0.6654  style_consist: -2.4463  content_consist: 0.0290  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 125/250 [07:01<05:17,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 130/250 [07:14<05:04,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.2233  rec_org: 4.4986  cls: 0.6299  style_consist: -2.2808  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 140/250 [07:39<04:31,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.2685  rec_org: 4.5046  cls: 0.4144  style_consist: -2.3868  content_consist: 0.0655  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 150/250 [08:05<04:08,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.1558  rec_org: 4.4580  cls: 0.4255  style_consist: -2.3294  content_consist: 0.0000  rec_grey: 0.0000\n","Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 160/250 [08:32<03:43,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.1014  rec_org: 4.6007  cls: 0.1780  style_consist: -2.3593  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 170/250 [08:57<03:17,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.1129  rec_org: 4.4137  cls: 0.4441  style_consist: -2.3759  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 175/250 [09:10<03:10,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 180/250 [09:24<02:57,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.0884  rec_org: 4.2982  cls: 0.4297  style_consist: -2.3657  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 190/250 [09:49<02:28,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.0136  rec_org: 4.2589  cls: 0.8495  style_consist: -2.2947  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 200/250 [10:15<02:04,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.0515  rec_org: 4.4541  cls: 0.9142  style_consist: -2.3074  content_consist: 0.0583  rec_grey: 0.0000\n","Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 210/250 [10:41<01:39,  2.49s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 4.0042  rec_org: 4.1851  cls: 0.2444  style_consist: -2.1925  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 220/250 [11:07<01:14,  2.47s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 3.9685  rec_org: 4.4719  cls: 0.2516  style_consist: -2.2200  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 225/250 [11:20<01:03,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Saving history model\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 230/250 [11:33<00:50,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 3.9538  rec_org: 4.1825  cls: 0.1346  style_consist: -2.3445  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 240/250 [11:59<00:24,  2.48s/it]"]},{"output_type":"stream","name":"stdout","text":["Train Stage 1: AE: \n","rec_rd: 3.9313  rec_org: 4.5582  cls: 0.2181  style_consist: -2.3423  content_consist: 0.0000  rec_grey: 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 250/250 [12:24<00:00,  2.98s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"FvHXLg8wwSHM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634048205320,"user_tz":-330,"elapsed":686,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"55c0b67d-5c76-4ea3-c42f-4b8a8adc6355"},"source":["!python -V"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.12\n"]}]},{"cell_type":"code","metadata":{"id":"SUQUYJI0x3Ca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634048369877,"user_tz":-330,"elapsed":423,"user":{"displayName":"RAJAT SHARMA","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13976417869986616281"}},"outputId":"557bfb1f-3879-4ec1-e685-a38ae9675410"},"source":["print(torch.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1.9.0+cu111\n"]}]},{"cell_type":"code","metadata":{"id":"2i7invwKfCa5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWQPAP8nfCWv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aomoXpZfCBT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnjSUkmcfB9B"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNT_sxMCfB41"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5L18qXZfB0p"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8KDkE90fBvr"},"source":[""],"execution_count":null,"outputs":[]}]}